{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRPJX2L1BS-S"
   },
   "source": [
    "# Lab 6: TD-Learning and Multi-armed Bandit\n",
    "In this lab, you will implement **Temporal-Difference (TD) learning**, a foundational reinforcement learning method that enables an agent to update its reward estimates incrementally based on the difference between predicted and observed rewards. You will apply TD learning in a **Pavlovian conditioning** environment, observing how the agent learns associations over time.\n",
    "\n",
    "You will also explore the **multi-armed bandit** problem, where an agent repeatedly selects among $ k $ actions, each providing a reward. The goal is to maximize the total reward over a series of selections. In this setup, each action’s reward is drawn from a Gaussian distribution with an unknown mean, creating an environment of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyZc969onm5S"
   },
   "outputs": [],
   "source": [
    "using_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Mm6g-kFn8mV",
    "outputId": "e741847c-3307-4491-f076-f3a94c1b4128"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import sys\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    sys.path.append('/content/gdrive/MyDrive/lab6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "H1Enc2obvWsl",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d757aa23172292814919873352f3e3a7",
     "grade": false,
     "grade_id": "cell-e7dcb07514771dfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_PSrqmxpxfK"
   },
   "source": [
    "# 1. TD Learning (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2ip4dnCEO9e"
   },
   "source": [
    "## 1.1 Introduction of TD Learning\n",
    "\n",
    "In reinforcement learning, we define the **state value function** $ V(s) $ as the expected return, or cumulative future reward, for being in state $ s $. This value represents the expected reward an agent can receive when starting from state $ s $ and following a particular policy. The purpose of defining $ V(s) $ is to help the agent evaluate states based on their expected long-term reward, guiding its decision-making process.\n",
    "\n",
    "Mathematically, $ V(s) $ can be expressed as:\n",
    "\n",
    "$$\n",
    "V(S_t) = \\mathbb{E} \\left[ \\sum_{i=t+1}^{t_{\\text{end}}} \\gamma^{i-(t+1)} r(S_i) \\right],\n",
    "$$\n",
    "\n",
    "where $ \\gamma $ is the **discount factor** $ (0 \\leq \\gamma < 1) $, which controls how much future rewards are weighted relative to immediate rewards. A lower $ \\gamma $ places less emphasis on future rewards. This expression can also be rewritten as a recursive formula called the **Bellman equation**:\n",
    "\n",
    "$$\n",
    "V(S_t) = \\mathbb{E} \\left[ r(S_{t+1}) + \\gamma V(S_{t+1}) \\right],\n",
    "$$\n",
    "\n",
    "The Bellman equation is crucial in reinforcement learning because it breaks down the value of the current state $ S_t $ into two parts: the immediate reward $ r(S_{t+1}) $ and the discounted value of the next state $ S_{t+1} $. This recursive formulation allows the agent to build value estimates incrementally, updating $ V(S_t) $ based on subsequent states without needing to calculate the full expected return for every step ahead.\n",
    "\n",
    "To improve our estimate of $ V(S_t) $ based on observed outcomes, we define the **Temporal-Difference (TD) error**:\n",
    "\n",
    "$$\n",
    "\\delta_{t+1} = r(S_{t+1}) + \\gamma V(S_{t+1}) - V(S_t),\n",
    "$$\n",
    "\n",
    "The TD error $ \\delta_{t+1} $ measures the difference between the current estimate $ V(S_t) $ and the updated reward based on what was actually observed $ r(S_{t+1}) + \\gamma V(S_{t+1}) $. This error term plays a vital role in learning because it quantifies how much the current value estimate needs adjustment.\n",
    "\n",
    "Finally, we adjust the value function $ V(S_t) $ incrementally using the TD error to refine our estimates in real-time:\n",
    "\n",
    "$$\n",
    "V_{\\text{new}}(S_t) = V_{\\text{old}}(S_t) + \\eta \\delta_{t+1},\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the **learning rate**, which controls how much influence the TD error has on the updated value estimate.\n",
    "\n",
    "All in all, the TD update process allows the agent to correct its value estimates as it experiences new states and rewards, helping it adapt to dynamic environments and become progressively more accurate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHK0zANIPDQO"
   },
   "source": [
    "## 1.2 Introduction of Pavlovian Conditioning Environment\n",
    "\n",
    "<center><img src=\"https://github.com/ZKBig/8803_GMM_Lab3/blob/main/pavlovian_conditioning_env.png?raw=true\" width=\"500\" height=\"320\"/></center>\n",
    "<!-- ![CA1_data](https://drive.google.com/uc?id=1y3QhKffqte7DbZmmXJFt2VC4zjbwrLGD) -->\n",
    "<center> Figure 1: Process of Pavlovian conditioning experiment. </center>\n",
    "\n",
    "Pavlovian conditioning is demonstrated a process where a neutral stimulus, like the sound of a bell, could trigger a salivery response in dogs through repeated pairings with food. Specifically, before conditioning, food can naturally cause the dog to salivate, which is an automatic response. Here, **food** is the **unconditioned stimulus (US)**, and **salivation** is the **unconditioned response (UR)**. The bell ringing does not initially cause any salivation, which is simply a neutral sound with no inherent meaning to the dog in this context. It is refered as **Neutral Stimulus (NS)**.\n",
    "\n",
    " During conditioning, the dog begins to associate the bell sound with the food, and salivation starts to occur even when the food is not present. Once this kind of association is established, the **bell ringing** becomes a **conditioned stimulus (CS)**. Even in the absence of food, the bell alone can trigger **salivation**, which is now referred to as the **conditioned response (CR)**.\n",
    "\n",
    " By applying TD learning to this Pavlovian conditioning setup, we allow the agent (dog) to develop predictive associations between the bell (**CS**) and the reward (**US**). This model mirrors the learning process observed in biological systems, where animals learn to anticipate rewards based on repeated exposure to cues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orAhLqDZuT59"
   },
   "source": [
    "## 1.3 Create the Pavlovian Conditioning Environment\n",
    "\n",
    "In this section, we create a class to represent the Pavlovian Conditoning environment, which allows the agent (dog) to interact with the environment in discrete episodes or trials. The detailed descriptions and instructions are listed as follows:\n",
    "\n",
    "- **Episode Structure**:\n",
    "  Each episode starts and ends in a special state called the **inter-trial interval (ITI)**. The ITI serves as a reset point between episodes. The value of this ITI state is fixed at zero, meaning the agent does not expect any reward from it. Episodes both begin from and terminate at the ITI state.\n",
    "\n",
    "- **State Transitions**:\n",
    "  Within an episode, the environment is represented as a **sequence of states** that the agent progresses through deterministically. Starting at **State 0**, the agent moves sequentially to **State 1**, then **State 2**, and so forth, until the episode concludes. Each state corresponds to a specific time step, forming a **tapped delay line representation** that enables the agent to keep track of the temporal progression within the episode.\n",
    "\n",
    "- **Presentation of Conditioned and Unconditioned Stimuli (CS and US)**:\n",
    "  During each episode, the agent is presented with two key stimuli: a **Conditioned Stimulus (CS)** and an **Unconditioned Stimulus (US)**. The **CS** is presented at exactly one-quarter of the total trial duration, serving as a cue for the upcoming reward. The **US** represents the reward, which is delivered a fixed interval after the **CS**. It is defined by the **reward_time** parameter, which specifies the delay between the **CS** and **US** presentations.\n",
    "\n",
    "The agent’s goal is to learn the expected reward value associated with each state in the episode. By learning these values, the agent can predict the timing and occurrence of the reward based on the cue (**CS**) provided earlier in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "r6VDCLXdRmlZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4a1748f086a54cec65542c088af8410",
     "grade": false,
     "grade_id": "cell-5962a4cc5e06d47c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class classical_conditioning():\n",
    "\n",
    "    def __init__(self, n_steps):\n",
    "\n",
    "        # Task variables\n",
    "        self.n_steps = n_steps\n",
    "        self.n_actions = 0\n",
    "\n",
    "        # Reward variables\n",
    "        self.reward_state = [0,0]\n",
    "        self.reward_magnitude = reward_magnitude\n",
    "        self.reward_probability = reward_probability\n",
    "        self.reward_time = reward_time\n",
    "\n",
    "        # Time step at which the conditioned stimulus is presented\n",
    "        self.cs_time = int(n_steps/4) - 1\n",
    "\n",
    "        # Create a state dictionary\n",
    "        self.create_state_dictionary()\n",
    "\n",
    "    def define_reward(self, reward_magnitude, reward_time):\n",
    "\n",
    "        \"\"\"\n",
    "        Determine reward state and magnitude of reward\n",
    "        \"\"\"\n",
    "        if reward_time >= self.n_steps - self.cs_time:\n",
    "            self.reward_magnitude = 0\n",
    "\n",
    "        else:\n",
    "            self.reward_magnitude = reward_magnitude\n",
    "            self.reward_state = [1, reward_time]\n",
    "\n",
    "    def get_outcome(self, current_state, action = 0):\n",
    "\n",
    "        \"\"\"\n",
    "        Determine next state and reward\n",
    "        \"\"\"\n",
    "        # Update state\n",
    "        if current_state < self.n_steps - 1:\n",
    "            next_state = current_state + 1\n",
    "        else:\n",
    "            # ITI state\n",
    "            next_state = 0\n",
    "\n",
    "        # Check for reward\n",
    "        if self.reward_state == self.state_dict[current_state]:\n",
    "            reward = self.reward_magnitude\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def create_state_dictionary(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This dictionary maps number of time steps/ state identities\n",
    "        in each episode to some useful state attributes:\n",
    "\n",
    "        state      - 0 1 2 3 4 5 (cs) 6 7 8 9 10 11 12 ...\n",
    "        is_delay   - 0 0 0 0 0 0 (cs) 1 1 1 1  1  1  1 ...\n",
    "        t_in_delay - 0 0 0 0 0 0 (cs) 1 2 3 4  5  6  7 ...\n",
    "        \"\"\"\n",
    "        d = 0\n",
    "\n",
    "        self.state_dict = {}\n",
    "        for s in range(self.n_steps):\n",
    "            if s <= self.cs_time:\n",
    "                self.state_dict[s] = [0,0]\n",
    "            else:\n",
    "                d += 1 # Time in delay\n",
    "                self.state_dict[s] = [1,d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x08wgmjTBLYy"
   },
   "source": [
    "Unless specified otherwise, we will use the following parameter values for the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEDXqSznBLYy"
   },
   "outputs": [],
   "source": [
    "reward_time = 10\n",
    "reward_magnitude = 10\n",
    "reward_probability = 1\n",
    "\n",
    "n_trials = 20000\n",
    "n_steps = 40\n",
    "\n",
    "gamma = 0.98  # temporal discount factor\n",
    "alpha = 0.001 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC6IPp_7Rml6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-emlshs3wsvI"
   },
   "source": [
    "## 1.4 TD-Learning for State-Value Estimation with Fixed-Reward\n",
    "In this session, you are asked to implement TD-learning to estimate the state-value function in the classical-conditioning world with guaranteed rewards, with a fixed magnitude, at a fixed delay after the CS. Note that TD-errors are also saved over the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPW--3PDgfix",
    "outputId": "dbe824ab-fc54-4a5f-dba2-de39f72b70de"
   },
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = classical_conditioning(n_steps)\n",
    "env.define_reward(reward_magnitude, reward_time)\n",
    "\n",
    "print(env.state_dict)\n",
    "print('Reward States:',env.reward_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOldcjl0I1VE"
   },
   "source": [
    "### **Implement learning process (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4PX2tJvw9Wd"
   },
   "outputs": [],
   "source": [
    "def initialize_values(n_steps, n_trials):\n",
    "    \"\"\"Initialize the value function V and TD error array.\"\"\"\n",
    "    V = np.zeros(n_steps)\n",
    "    TD_error = np.zeros((n_steps, n_trials))\n",
    "    return V, TD_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Je12-ge3w-Qy",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40cf7d60efe3b11d14864702976e8300",
     "grade": false,
     "grade_id": "cell-6f3b3fae9ec4c4bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_value_and_td_error(state, reward, next_state, V, gamma, alpha, TD_error, n, is_delay):\n",
    "    \"\"\"\n",
    "    Calculate TD error and update the value function.\n",
    "\n",
    "    Parameters:\n",
    "    - state (int): The current state index.\n",
    "    - reward (float): The reward received after transitioning to the next state.\n",
    "    - next_state (int): The index of the state following the current one.\n",
    "    - V (numpy.ndarray): The array representing the value estimates for each state.\n",
    "    - gamma (float): The discount factor, which weights future rewards.\n",
    "    - alpha (float): The learning rate, controlling the update magnitude.\n",
    "    - TD_error (numpy.ndarray): The 2D array storing the TD error values for each state and trial.\n",
    "    - n (int): The current trial index (used to store TD error for the trial).\n",
    "    - is_delay (bool/int): Indicator (1 or 0) to apply the update only if in a delay state.\n",
    "    \"\"\"\n",
    "    # please implement TD error and updated estimation of the current state\n",
    "    # in terms of the given expressions in Section 1.1.\n",
    "    # codes are no more than 2 lines\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAHL73BuxEFd"
   },
   "outputs": [],
   "source": [
    "V, TD_error = initialize_values(n_steps, n_trials)\n",
    "\n",
    "for n in range(n_trials):\n",
    "    state = 0\n",
    "    for t in range(n_steps):\n",
    "        # get the next_state index and reward of the current state\n",
    "        next_state, reward = env.get_outcome(state)\n",
    "        # whether the current state is in delay peorid\n",
    "        is_delay = env.state_dict[state][0]\n",
    "\n",
    "        # Call the function to update value and TD error\n",
    "        update_value_and_td_error(state, reward, next_state, V, gamma, alpha, TD_error, n, is_delay)\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1rcVnmtRmlo"
   },
   "source": [
    "### **Open question 1 (0.5 Point)**\n",
    "Plot the estimated value function for each state after learning has converged. Please also analyze the obtained plot to see wether it meets the expectation.\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "FDU36xaACo5t",
    "outputId": "a945924f-ddf0-4d89-b4be-794d371e0efd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(V,'o-')\n",
    "plt.xlabel('Time', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Value', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('Estimated Value Function', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feuLKYhbRmlu"
   },
   "source": [
    "### **Open question 2 (0.5 Point)**\n",
    "\n",
    "Plot the TD error over the whole learning process. Please also analyze the obtained plot to see wether it meets the expectation.\n",
    "\n",
    "**Hint:** Examine how the TD-error changes across iterations in the heatmap and line plot—Do specific points within each trial show a consistent decrease or increase in TD-error, and what might this indicate about the model’s ability to predict expected outcomes as learning progresses?\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "MBBLF0yQFxBN",
    "outputId": "c96cd68e-5941-4c90-ba80-77f4bcadf9f4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "# First plot: TD-error heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "indx = np.arange(0, n_trials, 400)\n",
    "plt.imshow(TD_error[:, indx], aspect='auto')\n",
    "plt.xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Time in trial', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('TD-error over learning (Heatmap)', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "\n",
    "# Second plot: TD-error over time in trial for different trials\n",
    "plt.subplot(1, 2, 2)\n",
    "indx = np.arange(0, n_trials, 1000)\n",
    "for ii, iindx in enumerate(indx):\n",
    "    plt.plot(np.arange(n_steps) - ii * 2, TD_error[:, iindx] + ii)\n",
    "\n",
    "plt.xlabel('Time in trial', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Trial number x TD-error', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('TD-error over learning (Line Plot)', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYQ3kNXkiGxH"
   },
   "source": [
    "## 1.5 TD-Learning for State-Value Estimation with Probabilistic Rewards\n",
    "\n",
    "In this session, you are asked to implement TD-learning in the context of probabilistic rewards. Determine a probability of reward delivery ($P(r) < 1$) and on each trial randomly determine whether or not to present a reward on that trial. Note that we keep reward magnitude constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cH_tXLovKtCT"
   },
   "source": [
    "### **Implement Learning process (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzcJ0Tu8BLY1"
   },
   "outputs": [],
   "source": [
    "# define the reward probability\n",
    "reward_probability = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "OFDqhFa4xnuN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6b6c3f1bb42f0e0984536a9a520fe36",
     "grade": false,
     "grade_id": "cell-aa78eb9a8deb9198",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "V, TD_error = initialize_values(n_steps, n_trials)\n",
    "\n",
    "for n in range(n_trials):\n",
    "    state = 0\n",
    "    for t in range(n_steps):\n",
    "        # please implement the reward delivery process with certain probability\n",
    "        # remeber to store all delivered reward magnitudes using rr\n",
    "        # codes are no more than 4 lines using np.random, if...else..., and env.define_reward(..,..)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        next_state, reward = env.get_outcome(state)\n",
    "        is_delay = env.state_dict[state][0]\n",
    "\n",
    "        update_value_and_td_error(state, reward, next_state, V, gamma, alpha, TD_error, n, is_delay)\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "k9QEvgdhJ1-h",
    "outputId": "3de2ff7f-e3b6-48f6-fa1f-75d539d35dd1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(V,'o-')\n",
    "plt.xlabel('Time', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Value', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('Estimated Value Function', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mpgz-d9qLb-t"
   },
   "source": [
    "### **Open Question 3 (0.5 Point)**\n",
    "\n",
    "Plot the TD error over the whole learning process. Please also analyze the obtained plot to see wether it meets the expectation.\n",
    "\n",
    "**Hint:** For analysis, please mainly focus on the different parts from the results in 1.4, and analyze the corresponding reasons.\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "lAxk6Fh4LOnN",
    "outputId": "ff054a17-99d2-43d2-d6f9-4936b38167b2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "# First plot: TD-error heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "indx = np.arange(0, n_trials, 400)\n",
    "plt.imshow(TD_error[:, indx], aspect='auto')\n",
    "plt.xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Time in trial', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('TD-error over learning (Heatmap)', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "\n",
    "# Second plot: TD-error over time in trial for different trials\n",
    "plt.subplot(1, 2, 2)\n",
    "indx = np.arange(0, n_trials, 1000)\n",
    "for ii, iindx in enumerate(indx):\n",
    "    plt.plot(np.arange(n_steps) - ii * 2, TD_error[:, iindx] + ii)\n",
    "\n",
    "plt.xlabel('Time in trial', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Trial number x TD-error', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('TD-error over learning (Line Plot)', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm2U3wu_YGS9"
   },
   "source": [
    "## 1.6 TD-Learning for State-Value Estimation with Variable Delays\n",
    "\n",
    "In this session, you are asked to implement TD-learning in the context of rewards presented at variable delays since **CS**. Use a set of discrete times since CS. On each trial, randomly select the delay between **CS** and **US**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1rFIA0JzcEz"
   },
   "source": [
    "### **Implement Learning process (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZahtXSNFCe_"
   },
   "outputs": [],
   "source": [
    "reward_times = np.arange(10,20,1)\n",
    "reward_probability = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "d2u2CtuRBLY2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7de730f5e8dc3d103b33eed2e1064f",
     "grade": false,
     "grade_id": "cell-ccc6852f0f049262",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = classical_conditioning(n_steps)\n",
    "V, TD_error = initialize_values(n_steps, n_trials)\n",
    "\n",
    "for n in range(n_trials):\n",
    "    state = 0\n",
    "    # please implement the reward delivery process with different delay time\n",
    "    # codes are no more than 5 lines using if...else..., np.random, and env.define_reward(..,..)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        next_state, reward = env.get_outcome(state)\n",
    "        is_delay  = env.state_dict[state][0]\n",
    "        update_value_and_td_error(state, reward, next_state, V, gamma, alpha, TD_error, n, is_delay)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "a7SjmHlPm8pr",
    "outputId": "74e843e1-2806-42f9-ffa7-8f552d5da743"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(V,'o-')\n",
    "plt.xlabel('Time', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Value', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('Estimated Value Function', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu4r18C4nHtg"
   },
   "source": [
    "### **Open question 4 (0.5 Point)**\n",
    "\n",
    "Plot the TD error over the whole learning process. Please also analyze the obtained plot to see wether it meets the expectation.\n",
    "\n",
    "**Hint:** For analysis, please mainly focus on the different parts from the results in 1.4, and analyze the corresponding reasons.\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "zpqg_0jwm_Nk",
    "outputId": "1c6017bd-bfa1-499a-9920-b78be8d04df4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "# First plot: TD-error heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "indx = np.arange(0, n_trials, 400)\n",
    "plt.imshow(TD_error[:, indx], aspect='auto')\n",
    "plt.xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Time in trial', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('TD-error over learning (Heatmap)', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "\n",
    "# Second plot: TD-error over time in trial for different trials\n",
    "plt.subplot(1, 2, 2)\n",
    "indx = np.arange(0, n_trials, 1000)\n",
    "for ii, iindx in enumerate(indx):\n",
    "    plt.plot(np.arange(n_steps) - ii * 2, TD_error[:, iindx] + ii)\n",
    "\n",
    "plt.xlabel('Time in trial', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Trial number x TD-error', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title('TD-error over learning (Line Plot)', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kx3pcwnnMI0"
   },
   "source": [
    "# 2. Multi-armed Bandit (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhaUx8QDqHEp"
   },
   "source": [
    "## 2.1 Introduction of Multi-armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-6k-UTVpUiz"
   },
   "source": [
    "Consider a learning problem where you repeatedly choose among\n",
    "$k$ different actions. Each choice results in a reward—a numerical value where higher values are better. The objective is to maximize the total reward over a fixed time, such as 1000 selections. This is known as the **k-armed bandit problem**, named after slot machines or \"one-armed bandits.\" In the multi-armed bandit variation, you face multiple \"machines\" (or actions), but can only choose one at a time. The challenge is to decide which machine to play—or which action to take—to maximize your total payout.\n",
    "\n",
    "For simplicity, we assume each action's reward follows a **Gaussian distribution** with an unknown mean and unit variance, referred to as the environment. The goal is to find the action with the highest average reward. To solve this optimization problem, we use an agent—an algorithm that learns from received rewards to inform its action choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOcpqEHiqMkg"
   },
   "source": [
    "## 2.2 Action Selection\n",
    "\n",
    "To maximize its reward, our agent first needs a way to choose which \"arm\" to pull at each step. The strategy it uses for selecting actions is called a **policy**, which is denoted as $ \\pi $. A simple option would be a random policy, where the agent picks an arm at random each time. However, this lacks intentionality and is unlikely to optimize the reward. Instead, we need a method to represent our beliefs about each arm’s reward potential. This is achieved with an **action-value function**, which estimates the expected cumulative reward for each action in a given state.\n",
    "\n",
    "We define the action-value function as:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) = \\mathbb{E} \\left[ \\sum_{i=t+1}^{t_{\\text{end}}} \\gamma^{i-(t+1)} r(S_i) \\right]\n",
    "$$\n",
    "\n",
    "In practice, the action-value function $ Q(S_t, A_t) $ is often represented as an array, where each action’s value is stored and updated based on experience.\n",
    "\n",
    "With action values defined, we can now use them to develop a policy. An intuitive choice is the **greedy policy**, which always selects the action with the highest current estimated value for each state:\n",
    "\n",
    "$$\n",
    "A_t = \\underset{a \\in \\mathcal{A}}{\\arg \\max} \\, Q(S_t, a)\n",
    "$$\n",
    "\n",
    "This approach chooses the action that maximizes the expected value $ Q(S_t, a) $ based on the current state $ S_t $.\n",
    "\n",
    "### **The Exploitation-Exploration Dilemma**\n",
    "A purely greedy approach, however, has a significant limitation: it may easily become trapped in local optima, repeatedly selecting an action that appears best without exploring alternatives that might yield higher rewards over time. This introduces the **exploitation-exploration dilemma**:\n",
    "\n",
    "- **Exploitation**: Choosing the action that currently seems best to maximize immediate reward.\n",
    "- **Exploration**: Trying other actions to gather more information, which may reveal better options over time.\n",
    "\n",
    "If the agent always \"plays it safe\" (exploitation), it risks missing out on potentially better rewards. Conversely, if it explores too much, it may fail to fully exploit the best option when it’s time to stop. Effective policies must balance these two aims to avoid getting stuck in local minima while maximizing total reward.\n",
    "\n",
    "A simple yet effective way to balance exploitation and exploration is the **$ \\epsilon $-greedy policy**. This policy extends the greedy approach by introducing a small probability $ \\epsilon $ of choosing an action at random rather than sticking with the current best choice:\n",
    "\n",
    "$$\n",
    "P(A_t = a) =\n",
    "        \\begin{cases}\n",
    "        1 - \\epsilon + \\frac{\\epsilon}{N}    & \\quad \\text{if } a = \\underset{a' \\in \\mathcal{A}}{\\arg\\max} \\; Q(S_t, a') \\\\\n",
    "        \\frac{\\epsilon}{N}        & \\quad \\text{otherwise}\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "where $N$ represents the total number of available actions in the action space. In other words, with probability $ 1 - \\epsilon $ (for $ \\epsilon \\in [0,1] $), the agent selects the greedy choice, and with probability $ \\epsilon $, it selects any action at random, including the greedy option.\n",
    "\n",
    "The $ \\epsilon $-greedy policy is popular due to its simplicity and effectiveness, providing a straightforward balance between exploitation and exploration. By allowing occasional exploration, it helps the agent avoid suboptimal rewards and discover better actions over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSZ0XcgmxbTx"
   },
   "source": [
    "### **Implement Epsilon-Greedy (2 Point)**\n",
    "You are aksed to implement the epsilon-greedy algorithm for deciding which action to take from a set of possible actions given their value function and a probability $\\epsilon$ of simply chosing one at random. You may find [`np.random.random`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html), [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) useful here.\n",
    "\n",
    "Also try how changing $\\epsilon$ influences our selection of the max value vs the others. At the extremes of its range (0 and 1), the $\\epsilon$-greedy policy reproduces two other policies. What are they?\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "deletable": false,
    "id": "mGT7fiinndCR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dd8767e90eef64ad1a502d29cd66ed8",
     "grade": false,
     "grade_id": "cell-43f8cc12973c704a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "330a1758-d5d3-4e62-864b-56eaa1105f07"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, epsilon):\n",
    "    \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
    "        (1-epsilon) and selects randomly with epsilon probability.\n",
    "\n",
    "    Args:\n",
    "        q (ndarray): an array of action values\n",
    "        epsilon (float): probability of selecting an action randomly\n",
    "\n",
    "    Returns:\n",
    "        int: the chosen action\n",
    "    \"\"\"\n",
    "    # write a boolean expression that determines if we should take the best action\n",
    "    be_greedy = np.random.random() > epsilon\n",
    "\n",
    "    if be_greedy:\n",
    "        # write an expression for selecting the best action from the action values\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        # write an expression for selecting a random action\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "q = [-2, 5, 0, 1]\n",
    "epsilon = 0.1\n",
    "\n",
    "# Visualize\n",
    "plot_choices(q, epsilon, epsilon_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUmc9FNyzee1"
   },
   "source": [
    "## 2.3 Learning From Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5pW_3z70Q47"
   },
   "source": [
    "With a policy for deciding actions, the next step is to learn from each action’s outcome. One approach is to keep a complete record of all past rewards for each action and calculate their averages. However, for long episodes, this becomes computationally expensive due to storage and repeated calculations. Instead, we can use an incremental (or streaming) mean update:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{1}{n_t} \\left( r(S_{t+1}) - Q(S_t, A_t) \\right)\n",
    "$$\n",
    "\n",
    "where $ n_t $ is the number of times action $ A_t $ has been selected in state $ S_t $ by time $ t $. This approach updates the mean incrementally with each new reward, minimizing computational cost.\n",
    "\n",
    "To simplify further and avoid tracking $ n_t $, we introduce a general parameter $ \\alpha $, called the **learning rate**, which controls the influence of each new reward on the estimate. The update rule then becomes:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( r(S_{t+1}) - Q(S_t, A_t) \\right)\n",
    "$$\n",
    "\n",
    "This form makes the update process more flexible, with $ \\alpha $ determining how much each new reward adjusts the estimated value $ Q(S_t, A_t) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vnacAkj0qd4"
   },
   "source": [
    "### **Updating Action Values (0.5 Point)**\n",
    "You are asked to implement the action-value update rule above. The function will take in the action-value function represented as an array $q$, the action taken, the reward received, and the learning rate, **$\\alpha$**. The function will return the updated value for the selection action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "JoB_7GejyEZF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a023d33b8d0a99720fea100a7008501",
     "grade": false,
     "grade_id": "cell-be5a55a41fdc5522",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "493b0cfb-7a5d-48b9-ea45-9dfecb7f674f"
   },
   "outputs": [],
   "source": [
    "def update_action_value(q, action, reward, alpha):\n",
    "    \"\"\" Compute the updated action value given the learning rate and observed\n",
    "    reward.\n",
    "\n",
    "    Args:\n",
    "        q (ndarray): an array of action values\n",
    "        action (int): the action taken\n",
    "        reward (float): the reward received for taking the action\n",
    "        alpha (float): the learning rate\n",
    "\n",
    "    Returns:\n",
    "        float: the updated value for the selected action\n",
    "    \"\"\"\n",
    "\n",
    "    # Write an expression for the updated action value\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "q = [-2, 5, 0, 1]\n",
    "action = 2\n",
    "print(f\"Original q({action}) value = {q[action]}\")\n",
    "\n",
    "# Update action\n",
    "q[action] = update_action_value(q, 2, 10, 0.01)\n",
    "print(f\"Updated q({action}) value = {q[action]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZxgDSmA1Ons"
   },
   "source": [
    "## 2.4 Training Multi-armed Bandits (2 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izuQ1oza1W2w"
   },
   "source": [
    "Now that we have both a policy and a learning rule, we can combine these to solve our original multi-armed bandit task. Recall that we have some number of arms that give rewards drawn from Gaussian distributions with unknown mean and unit variance, and our goal is to find the arm with the highest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "PhhdBzl91ACi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7808ffc3ddb2a3332fdf032614af7f5",
     "grade": false,
     "grade_id": "cell-79a80bb03bb5bbc0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_armed_bandit(n_arms, epsilon, alpha, n_steps):\n",
    "    \"\"\" A Gaussian multi-armed bandit using an epsilon-greedy policy. For each\n",
    "    action, rewards are randomly sampled from normal distribution, with a mean\n",
    "    associated with that arm and unit variance.\n",
    "\n",
    "    Args:\n",
    "        n_arms (int): number of arms or actions\n",
    "        epsilon (float): probability of selecting an action randomly\n",
    "        alpha (float): the learning rate\n",
    "        n_steps (int): number of steps to evaluate\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary containing the action values, actions, and rewards from\n",
    "        the evaluation along with the true arm parameters mu and the optimality of\n",
    "        the chosen actions.\n",
    "    \"\"\"\n",
    "    # Gaussian bandit parameters\n",
    "    mu = np.random.normal(size=n_arms)\n",
    "\n",
    "    # Evaluation and reporting state\n",
    "    q = np.zeros(n_arms)\n",
    "    qs = np.zeros((n_steps, n_arms))\n",
    "    rewards = np.zeros(n_steps)\n",
    "    actions = np.zeros(n_steps)\n",
    "    optimal = np.zeros(n_steps)\n",
    "\n",
    "    # Run the bandit\n",
    "    for t in range(n_steps):\n",
    "\n",
    "        # Choose an action\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        actions[t] = action\n",
    "\n",
    "        # Compute rewards for all actions\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Observe the reward for the chosen action\n",
    "        reward = all_rewards[action]\n",
    "        rewards[t] = reward\n",
    "\n",
    "        # Was it the best possible choice?\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        optimal[t] = action == optimal_action\n",
    "\n",
    "        # Update the action value\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        qs[t] = q\n",
    "\n",
    "    results = {\n",
    "        'qs': qs,\n",
    "        'actions': actions,\n",
    "        'rewards': rewards,\n",
    "        'mu': mu,\n",
    "        'optimal': optimal\n",
    "      }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaxJYxAT10RN"
   },
   "source": [
    "We can use our multi-armed bandit method to evaluate how our epsilon-greedy policy and learning rule perform at solving the task. First we will set our environment to have 10 arms and our agent parameters to $\\epsilon=0.1$ and $\\alpha=0.01$. In order to get a good sense of the agent's performance, we will run the episode for 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "mioq371Q1uRa",
    "outputId": "ec57b43e-777d-43be-afc3-2f8a492c4d60"
   },
   "outputs": [],
   "source": [
    "# set for reproducibility, comment out / change seed value for different results\n",
    "np.random.seed(1)\n",
    "n_arms = 10\n",
    "epsilon = 0.1\n",
    "alpha = 0.01\n",
    "n_steps = 1000\n",
    "\n",
    "results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Plot for observed rewards\n",
    "ax1.plot(results['rewards'])\n",
    "ax1.set(title=f'Observed Reward ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
    "        xlabel='Step', ylabel='Reward')\n",
    "ax1.title.set_weight('bold')\n",
    "ax1.xaxis.label.set_weight('bold')\n",
    "ax1.yaxis.label.set_weight('bold')\n",
    "ax1.tick_params(axis='both', which='both', labelsize=10, width=2)\n",
    "ax1.spines['top'].set_linewidth(2)\n",
    "ax1.spines['bottom'].set_linewidth(2)\n",
    "ax1.spines['left'].set_linewidth(2)\n",
    "ax1.spines['right'].set_linewidth(2)\n",
    "\n",
    "# Plot for action values\n",
    "ax2.plot(results['qs'])\n",
    "ax2.set(title=f'Action Values ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
    "        xlabel='Step', ylabel='Value')\n",
    "ax2.title.set_weight('bold')\n",
    "ax2.xaxis.label.set_weight('bold')\n",
    "ax2.yaxis.label.set_weight('bold')\n",
    "ax2.tick_params(axis='both', which='both', labelsize=10, width=2)\n",
    "ax2.spines['top'].set_linewidth(2)\n",
    "ax2.spines['bottom'].set_linewidth(2)\n",
    "ax2.spines['left'].set_linewidth(2)\n",
    "ax2.spines['right'].set_linewidth(2)\n",
    "\n",
    "# Legend for action values plot\n",
    "ax2.legend(range(n_arms))\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yivFICpu2JZj"
   },
   "source": [
    "Alright, we got some rewards that are kind of all over the place, but the agent seemed to settle in on the first arm as the preferred choice of action relatively quickly. Let's see how well we did at recovering the true means of the Gaussian random variables behind the arms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcAJv5qvAnVq"
   },
   "source": [
    "### **Comparison Between the Estimated Means and Ground Truth**\n",
    "\n",
    "Now, let us compare the estimated values of means and the ground truth by running the following codes. We can see that the algorithm finds a very good estimate for action 0, but most if the others are not great. In fact, we can see the effect of the local maxima trap at work -- the greedy part of our algorithm locked onto action 0, which is actually the 2nd best choice to action 6. Since these are the means of Gaussian random variables, we can see that the overlap between the two would be quite high, so even if we did explore action 6, we may draw a sample that is still lower than our estimate for action 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "2WBInBffA8Qs",
    "outputId": "8ef5bce3-6552-45fe-ec51-3bd8e66b4099"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(results['mu'], '-o', label='latent')\n",
    "plt.plot(results['qs'][-1], '-o', label='learned')\n",
    "plt.xlabel('action', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('value', fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.title(f'$\\epsilon$={epsilon}, $\\\\alpha$={alpha}', fontsize=12, fontweight='bold')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRhbj1JiBkQD"
   },
   "source": [
    "We can further try different combination of $\\epsilon$ (exploitation-exploration tradeoff) and $\\alpha$ to see if we can obtain better results. Note that due to the stochastic nature of both our rewards and our policy, a single trial run isn't sufficient to give us this information-which combination is better. Let's run mulitple trials and compare the average performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYCDAAQrCQcu"
   },
   "source": [
    "### **Explore $\\epsilon$ (0.5 Point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytkdv8TP5FlF"
   },
   "source": [
    "\n",
    "First we will look at different values for $\\epsilon \\in [0.0, 0.1, 0.2]$ to a fixed $\\alpha=0.1$. We will run 200 trials as a nice balance between speed and accuracy. Please run the following codes and analyze the results.\n",
    "\n",
    "**Hint:** For analysis, please identify the values of $\\epsilon$ that optimize performance in the initial stage of learning and those that enhance performance in later stages, providing a comparative insight across the learning progression.\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "t9CEzXeOA1a1",
    "outputId": "13310362-0bfc-4073-9e2d-61c5d57ae4da"
   },
   "outputs": [],
   "source": [
    "# set for reproducibility, comment out / change seed value for different results\n",
    "np.random.seed(1)\n",
    "\n",
    "epsilons = [0.0, 0.1, 0.2]\n",
    "alpha = 0.1\n",
    "n_trials = 200\n",
    "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    for n in range(n_trials):\n",
    "        results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
    "        trial_rewards[i, n] = results['rewards']\n",
    "        trial_optimal[i, n] = results['optimal']\n",
    "\n",
    "labels = [f'$\\epsilon$={e}' for e in epsilons]\n",
    "fixed = f'$\\\\alpha$={alpha}'\n",
    "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG4K7EjsCjuj"
   },
   "source": [
    "### **Explore learnign rate (0.5 Point)**\n",
    "We can also do the same for the learning rates. We will evaluate $\\alpha \\in [0.01, 0.1, 1.0]$ to a fixed $\\epsilon=0.1$. Please run the following codes and analyze the results.\n",
    "\n",
    "**Hint:** You can compare the results for three different $\\alpha$ values across the entire learning process, highlighting which value consistently yields the best performance and which demonstrates the least favorable outcomes.\n",
    "\n",
    "<span style=\"color:red\">Your answer:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "S2ecdhFHCdKh",
    "outputId": "5a54c0fb-a04d-48a6-a2ac-084265558d6c"
   },
   "outputs": [],
   "source": [
    "# set for reproducibility, comment out / change seed value for different results\n",
    "np.random.seed(1)\n",
    "\n",
    "epsilon = 0.1\n",
    "alphas = [0.01, 0.1, 1.0]\n",
    "n_trials = 200\n",
    "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    for n in range(n_trials):\n",
    "        results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
    "        trial_rewards[i, n] = results['rewards']\n",
    "        trial_optimal[i, n] = results['optimal']\n",
    "\n",
    "labels = [f'$\\\\alpha$={a}' for a in alphas]\n",
    "fixed = f'$\\epsilon$={epsilon}'\n",
    "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79d836d55095f77eb3114caa72322ed2",
     "grade": true,
     "grade_id": "cell-d09d892c3c6795a9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
