{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify that we want our tensors on the CPU/GPU and in float32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.mnist.MNIST('./data', transform=transforms.ToTensor(), train=True)\n",
    "val_dataset = datasets.mnist.MNIST('./data', transform=transforms.ToTensor(), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_AE(nn.Module):\n",
    "    def __init__(self, width, height):\n",
    "        super(MNIST_AE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride = 1), # 26x26x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 13x13x16\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride = 1), # 11x11x8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 5x5x8\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(5*5*8, 3)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 5*5*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (8, 5, 5)),\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2), # 11x11x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=3, padding=3, output_padding=1), # 28x28x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, image_tensor):\n",
    "        encoded = self.encoder(image_tensor)\n",
    "        flattened = self.flatten(encoded)\n",
    "        latent = self.fc(flattened)\n",
    "        decoded = self.decoder(latent)\n",
    "        return decoded, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded shape: torch.Size([1, 1, 28, 28])\n",
      "Latent shape: torch.Size([1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "mnist_ae = MNIST_AE(28, 28).to(device)\n",
    "decoded, latent = mnist_ae.forward(train_dataset[0][0].to(device).reshape(1, 1, 28, 28))\n",
    "print(f\"Decoded shape: {decoded.shape}\")\n",
    "print(f\"Latent shape: {latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(reconstructed, original):\n",
    "    return torch.mean((original - reconstructed)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2311, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss(train_dataset[0][0].to(device).reshape(1, 1, 28, 28), decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JosephYu/opt/anaconda3/envs/nbd_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train_model(model, \n",
    "                train_dataset, \n",
    "                val_dataset,\n",
    "                objective,\n",
    "                regularizer=None,\n",
    "                num_epochs=100, \n",
    "                lr=0.1,\n",
    "                momentum=0.9,\n",
    "                lr_step_size=25,\n",
    "                lr_gamma=0.9):\n",
    "    # progress bars\n",
    "    pbar = trange(num_epochs)\n",
    "    pbar.set_description(\"---\")\n",
    "    inner_pbar = trange(len(train_dataset))\n",
    "    inner_pbar.set_description(\"Batch\")\n",
    "\n",
    "    # data loaders for train and validation\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "    dataloaders = dict(train=train_dataloader, val=val_dataloader)\n",
    "\n",
    "    # use standard SGD with a decaying learning rate\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=lr, \n",
    "                          momentum=momentum)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    \n",
    "    # Keep track of the best model\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e8\n",
    "\n",
    "    # Track the train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            # set model to train/validation as appropriate\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                inner_pbar.reset()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            # track the running loss over batches\n",
    "            running_loss = 0\n",
    "            running_size = 0\n",
    "            for _, (data, _) in enumerate(dataloaders[phase]):\n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "                        # compute the model output and loss\n",
    "                        output_t, _ = model(data)\n",
    "                        loss_t = objective(output_t, data)\n",
    "                        # only add the regularizer in the training phase\n",
    "                        if regularizer is not None:\n",
    "                            loss_t += regularizer(model)\n",
    "\n",
    "                        # take the gradient and perform an sgd step\n",
    "                        loss_t.backward()\n",
    "                        optimizer.step()\n",
    "                    inner_pbar.update(1)\n",
    "                else:\n",
    "                    # just compute the loss in validation\n",
    "                    output_t, _ = model(data)\n",
    "                    loss_t = objective(output_t, data)\n",
    "\n",
    "                assert torch.isfinite(loss_t)\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "            \n",
    "            # compute the train/validation loss and update the best\n",
    "            # model parameters if this is the lowest validation loss yet\n",
    "            running_loss /= running_size\n",
    "            if phase == \"train\":\n",
    "                train_losses.append(running_loss)\n",
    "            else:\n",
    "                val_losses.append(running_loss)\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.set_description(\"Epoch {:03} Train {:.4f} Val {:.4f}\"\\\n",
    "                             .format(epoch, train_losses[-1], val_losses[-1]))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return torch.tensor(train_losses), torch.tensor(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an GLM with random initial weights.\n",
    "torch.manual_seed(0)\n",
    "mnist_ae = MNIST_AE(28, 28).to(device)\n",
    "\n",
    "# Fit the GLM\n",
    "print(\"Training Autoencoder...\")\n",
    "train_losses, val_losses = \\\n",
    "    train_model(mnist_ae, \n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                reconstruction_loss,\n",
    "                num_epochs=10\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Solution:\n",
    "    def wallsAndGates(self, rooms: list) -> None:\n",
    "        \"\"\"\n",
    "        Do not return anything, modify rooms in-place instead.\n",
    "        \"\"\"\n",
    "        self.inf = 2**31 - 1\n",
    "        queue = collections.deque()\n",
    "        seen = set()\n",
    "\n",
    "        N = len(rooms)\n",
    "        M = len(rooms[0])\n",
    "\n",
    "        dist_to_gate = [[self.inf for _ in range(M)] for _ in range(N)]\n",
    "\n",
    "        for r in range(N):\n",
    "            for c in range(M):\n",
    "                if rooms[r][c] == 0:\n",
    "                    queue.append((r,c, 0))\n",
    "                    seen.add((r,c))\n",
    "                elif rooms[r][c] == -1:\n",
    "                    dist_to_gate[r][c] = -1\n",
    "        \n",
    "        while queue:\n",
    "            r, c, dist = queue.popleft()\n",
    "\n",
    "            dist_to_gate[r][c] = dist\n",
    "        \n",
    "            for n_r, n_c in {(r+1, c), (r-1, c), (r, c+1), (r, c-1)}:\n",
    "                if n_r >= 0 and n_c >= 0 and n_r < N and n_c < M and (n_r, n_c) not in seen and rooms[n_r][n_c] == self.inf:\n",
    "                    seen.add((n_r, n_c))\n",
    "                    queue.append((n_r, n_c, dist+1))\n",
    "        \n",
    "        return dist_to_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, -1, 0, 1], [2, 2, 1, -1], [1, -1, 2, -1], [0, -1, 3, 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, -1, 0, 1], [2, 2, 1, -1], [1, -1, 2, -1], [0, -1, 3, 4]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rooms = [[2147483647,-1,0,2147483647],[2147483647,2147483647,2147483647,-1],[2147483647,-1,2147483647,-1],[0,-1,2147483647,2147483647]]\n",
    "\n",
    "sol = Solution()\n",
    "sol.wallsAndGates(rooms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
