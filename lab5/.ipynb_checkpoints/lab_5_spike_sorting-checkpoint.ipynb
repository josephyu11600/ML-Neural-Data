{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aee21197a0767bbe3f7458345927cecc",
     "grade": false,
     "grade_id": "cell-79dbbdc3c797fb5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 5: Spike Sorting\n",
    "\n",
    "In this lab, you'll implement a spike sorting algorithm on a synthetic dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae05ab2b99606b607a019f81f9ad70db",
     "grade": false,
     "grade_id": "cell-0590835f349b1a57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import PyTorch modules\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# Some helper utilities\n",
    "from tqdm.auto import trange\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# !pip install neo elephant --quiet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import sys\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    sys.path.append('/content/gdrive/MyDrive/CSE8803_Labs/')\n",
    "    test_points = torch.load(\"/content/gdrive/MyDrive/CSE8803_Labs/test_points.pt\", map_location=device)\n",
    "else:\n",
    "    test_points = torch.load(\"./test_points.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "711aa84e05e00b91f5ed9d54555feb9d",
     "grade": false,
     "grade_id": "cell-f7a29441b8f20bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_syn_sorting, generate_templates, plot_model, plot_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99909cd266e606c651b762c28600c8df",
     "grade": false,
     "grade_id": "cell-1bb7ea40d590441a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Spike Sorting by Deconvolution\n",
    "\n",
    "In this part of the lab you'll use those cross-correlation and convolution operations to implement the spike sorting algorithm. We'll apply the algorithm to a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee016c8476b3bb2c1a12f93dcca5eebb",
     "grade": false,
     "grade_id": "cell-61f4975fbd43d4c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a larger dataset with a multiple channels and neurons.\n",
    "T = 100000 # number of time samples\n",
    "N = 32      # number of channels\n",
    "D = 81      # duration of a spike (in samples)\n",
    "K = 10      # multiple neurons\n",
    "\n",
    "# Generate random templates, amplitudes, and noisy data.\n",
    "# `templates` are NxCxD and `amplitudes` are NxT\n",
    "torch.manual_seed(0)\n",
    "print(\"Simulating data. This could take a few seconds!\")\n",
    "true_templates, true_amplitudes, data = generate_syn_sorting(T, N, D, K)\n",
    "plot_model(true_templates, true_amplitudes[:, :2000], data[:,:2000], lw=1, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4f2a7eaa8b7c9a15cc05cd1807cf1ea",
     "grade": false,
     "grade_id": "cell-8ce3bb807eeded3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate another set of random templates and amplitudes to seed the model\n",
    "torch.manual_seed(1)\n",
    "templates = generate_templates(N, D, K)\n",
    "amplitudes = torch.zeros((K, T))\n",
    "noise_std = 1.0\n",
    "\n",
    "# Copy the tensors to the GPU (if available)\n",
    "templates = templates.to(device)\n",
    "amplitudes = amplitudes.to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9789abdc3aa3217134f474a718dab59c",
     "grade": false,
     "grade_id": "cell-ba575d7b4bf4b9c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Compute the Log Likelihood\n",
    "\n",
    "One of the most awesome features of PyTorch is its `torch.distributions` package. See the docs [here](https://pytorch.org/docs/stable/distributions.html). It contains objects for many of our favorite distributions, and has convenient functions for computing log probabilities (with `d.log_prob()` where `d` is a `Distribution` object), sampling (`d.sample()`), computing the entropy (`d.entropy()`), etc. These functions broadcast as you'd expect (unlike `scipy.stats`!), and they're designed to work with automatic differentiation.  More on that another day...\n",
    "\n",
    "For now, you'll use `MultivariateNormal` to compute the log likelihood of the data given the template and amplitudes, $\\log p(\\mathbf{X} \\mid \\mathbf{A}, \\mathbf{W})$.  To do that, you'll convolve the amplitudes and templates to get the mean value of $\\mathbf{X}$, then you'll use the `log_prob` function to evaluate the likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d0c405663a3c3dcd4034dda12a7757b",
     "grade": false,
     "grade_id": "cell-9cb6c32885afcafe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(templates, amplitudes, data, noise_std):\n",
    "    \"\"\"Evaluate the log likelihood\"\"\"\n",
    "    K, N, D = templates.shape\n",
    "    _, T = data.shape\n",
    "    \n",
    "    # Compute the model prediction by convolving the amplitude and templates\n",
    "    # new_amplitudes = torch.filp(torch.permute(..., ...), ...)\n",
    "    # pred = F.conv1d(input=..., weight=...,padding=...)\n",
    "    # pred = torch.squeeze(pred)\n",
    "    # dis = torch.distributions.multivariate_normal.MultivariateNormal(...)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Evaluate the log probability using dist.Normal\n",
    "    ll = torch.sum(dis.log_prob(data.T))\n",
    "\n",
    "    # Return the log probability normalized by the data size\n",
    "    return ll / (N * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8714d01b918927973d1ce4c42ad3e0b0",
     "grade": false,
     "grade_id": "cell-f0f493ed65504b59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your answer\n",
    "ll = log_likelihood(templates, amplitudes, data, noise_std)\n",
    "assert ll.isclose(test_points[\"q1\"], atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9d765da905c9c0da912864cfc203c4e",
     "grade": false,
     "grade_id": "cell-f92db3455f509a1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "For the following sections, you'll use the `conv1d` function in the `torch.nn.functional` package. We've already imported that package with the shorthand name `F` so that you can call the function with `F.conv1d(...)`. Take a look at its documentation [here](https://pytorch.org/docs/stable/nn.functional.html?highlight=conv1d#torch.nn.functional.conv1d), as well as the corresponding documentation for the `torch.nn.Conv1d` object, which implements a convolutional layer for a neural network. \n",
    "\n",
    "**Remember that `conv1d` actually performs a cross-correlation!** \n",
    "\n",
    "Let $\\mathbf{A} \\in \\mathbb{R}^{B \\times K \\times T}$ denote the signal/input and $\\mathbf{W} \\in \\mathbb{R}^{N \\times K \\times D}$ denote the filter/weights (note that the axes are permuted relative to our mathematical notes), and let $\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times T - D + 1}$ denote the output. Then the `conv1d` function implements the cross-correlation, \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{b,n,t} = \\sum_{k = 1}^{K} \\sum_{d=1}^D a_{b,k,t+d-1} w_{n,k,d}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for $b=1,\\ldots,B$, $n=1,\\ldots,N$, and $t=1,\\ldots,T-D+1$. \n",
    "\n",
    "By default the output only contains the \"valid\" portion of the convolution; i.e. the $T-D+1$ samples where the inputs and weights completely overlap. If you want the \"full\" output, you have to call `F.conv1d(input, weights, padding=D-1)`. This pads the input with $D-1$ zeros at the beginning and end so that the resulting output is length $T + D - 1$. Depending your application, you may want the first $T$ or the last $T$ entries in this array. When in doubt, try both and see!\n",
    "\n",
    "Use `conv1d` to implement a 1d **convolution**. Remember that you can do it by cross-correlation as long as you flip your weights along the last axis. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e0240cb59b66370e2e2f55df4735cd0",
     "grade": false,
     "grade_id": "cell-8bf468c6c1fd9ad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Compute the residual\n",
    "\n",
    "Compute the residual for a specified neuron by subtracting the convolved amplitudes and templates for all the other neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4c84dd7d820411c12af8e372af7bc8c",
     "grade": false,
     "grade_id": "cell-f5408b6fb2c04a28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_residual(neuron, templates, amplitudes, data):\n",
    "    K, C, D = templates.shape\n",
    "\n",
    "    # Compute the predicted value of the data by \n",
    "    # convolving the amplitudes and the templates for all\n",
    "    # neurons except the specified one.\n",
    "    \n",
    "    indexes_without_neuron = torch.cat([torch.arange(neuron), torch.arange(neuron+1, K)])\n",
    "\n",
    "    # new_templates = torch.filp(torch.permute(..., ...), ...)\n",
    "    # pred = F.conv1d(input=..., weight=...,padding=...)[:,:,0:T]\n",
    "    # pred = torch.squeeze(pred)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Return the data minus the predicted value given other neurons\n",
    "    return data - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6a2aadd668fd1154dec9380ed35bb77",
     "grade": false,
     "grade_id": "cell-e6680f85eb757620",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your answer\n",
    "residual = compute_residual(0, templates, amplitudes, data)\n",
    "assert residual[16, [0, 1234, -1]].allclose(test_points[\"q2\"], atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58a98aaec90406237d22a0904e701b51",
     "grade": false,
     "grade_id": "cell-8260e40e35e44cab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Compute the score\n",
    "\n",
    "Let the \"score\" for neuron $k$ be the cross-correlation of the residual and its template. Compute it using `conv1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abe1a4b28a02962aac373c0219604663",
     "grade": false,
     "grade_id": "cell-ec3c8a6b5c7cf5d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_score(neuron, templates, amplitudes, data):\n",
    "    K, N, D = templates.shape\n",
    "    T = data.shape[1]\n",
    "\n",
    "    # First get the residual\n",
    "    residual = compute_residual(neuron, templates, amplitudes, data)\n",
    "\n",
    "    # score = F.conv1d(input=..., weight=...,padding=...)[:, :, D-1:]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    return torch.squeeze(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d6e3783c4425ec838772d1fbf5c0c91",
     "grade": false,
     "grade_id": "cell-a1c1dddc3fd094e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your answer\n",
    "score = compute_score(0, templates, amplitudes, data)\n",
    "assert score[[0, 1234, -1]].allclose(test_points[\"q3\"], atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c452758e967de3934b67c002e3407b40",
     "grade": false,
     "grade_id": "cell-bc322532913eef8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Update the amplitudes using `find_peaks`\n",
    "\n",
    "Our next step is to update the amplitudes given the scores. We'll use a simple heuristic as described in the course: use `scipy.signal.find_peaks` to find peaks in the score that are separated by a distance of at least $D$ samples and at least a height of $\\sigma^2 \\lambda$, where $\\sigma$ is the standard deviation of the noise and $\\lambda$ is the amplitude rate hyperparameter. \n",
    "\n",
    "Note that this solution does not guarantee that the resulting nonzero amplitudes will be separated by at least $D$ time steps! In practice, we can enforce this constraint via the following heuristic: after solving for the optimal amplitudes, use the `scipy.signal.find_peaks` function to keep only a subset of nonzero amplitudes that are separated by a distance of $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41ea1bcb21b782f726a325e162863292",
     "grade": false,
     "grade_id": "cell-03b7631695750ad5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def _update_amplitude(neuron, templates, amplitudes, data, \n",
    "                      noise_std=1.0, amp_rate=5.0):\n",
    "    K, N, D = templates.shape\n",
    "    T = data.shape[1]\n",
    "\n",
    "    # Compute the score and convert it to a numpy array.\n",
    "    score = compute_score(neuron, templates, amplitudes, data).to(\"cpu\")\n",
    "    \n",
    "    # Initialize the new amplitudes a_k for this neuron\n",
    "    new_amplitude = torch.zeros(T, device=device)\n",
    "    \n",
    "    # peaks, props = scipy.signal.find_peaks(..., distance=..., height=...)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Convert the peak heights to a tensor\n",
    "    heights = torch.tensor(props['peak_heights'], \n",
    "                           dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute the new amplitude for this neuron.\n",
    "    new_amplitude[peaks] = heights\n",
    "\n",
    "    ###\n",
    "        \n",
    "    return new_amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aecc976bc70129f455c386b503e7028f",
     "grade": false,
     "grade_id": "cell-864fe50a4929ca60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your anseer\n",
    "amplitudes[0] = _update_amplitude(0, templates, amplitudes, data)\n",
    "assert torch.allclose(amplitudes[0][[136, 253, 898, -1]].to(\"cpu\"), \n",
    "                      test_points[\"q4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4c4b4cb2413f7716d5f56226a4391b0",
     "grade": false,
     "grade_id": "cell-98f882a7b3d265c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Update the templates \n",
    "Our last step is to update the template for a given neuron by projecting the _target_ $\\overline{\\mathbf{R}} \\in \\mathbb{R}^{N \\times D}$. The target is the sum of scaled residuals at the times of spikes in the amplitudes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\overline{\\mathbf{R}} = \\sum_t a_{k,t} \\mathbf{R}_{:,t:t+D}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{R} \\in \\mathbb{R}^{N \\times T}$ denotes the residual for neuron $n$. \n",
    "\n",
    "To get the template, project $\\overline{\\mathbf{R}}$ onto $\\mathcal{S}_K^{(N,D)}$, the set of rank-$K$, unit-norm, $N \\times D$ matrices, using the SVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1280767a0df834067af597c3f81fab55",
     "grade": false,
     "grade_id": "cell-e16336939bdfc10f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def _update_template(neuron, templates, amplitudes, data, template_rank=1):\n",
    "    K, N, D = templates.shape\n",
    "    T = data.shape[1]\n",
    "\n",
    "    # Initialize the new template\n",
    "    new_template = torch.zeros((N, D), device=device)\n",
    "\n",
    "    # Check if the factor is used. If not, generate a random new one.\n",
    "    if amplitudes[neuron].sum() < 1:\n",
    "        target = generate_templates(N, D, 1)[0]\n",
    "\n",
    "    else:\n",
    "        # Get the residual using the function you wrote above\n",
    "        residual = compute_residual(neuron, templates, amplitudes, data)\n",
    "\n",
    "        # Compute the \\bar{R} based on the above formula: \\bar{R} = \\sum_{t}aR\n",
    "        # bar_R = ...\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    ###\n",
    "    # Project the target onto the set of normalized rank-K templates using \n",
    "    # `torch.linalg.svd` and `torch.linalg.norm`.\n",
    "\n",
    "    # U, S, VT = torch.linalg.svd(...)\n",
    "    # S = torch.diag(S)\n",
    "    # S_K = S[...]\n",
    "    # S_K_norm = ...\n",
    "    # new_template = U[...] @ S_K_norm @ VT[...]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return new_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69548bc1e6eec39297572f07ee95f841",
     "grade": false,
     "grade_id": "cell-29ec082b9d31974c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your answer\n",
    "\n",
    "# Set amplitudes using previous cell output\n",
    "# so the template update code is executed\n",
    "\n",
    "templates[0] = _update_template(0, templates, amplitudes, data)\n",
    "assert torch.linalg.norm(templates[0], 'fro').isclose(\n",
    "    torch.tensor(1.0), atol=1e-4)\n",
    "assert templates[0][16, 44].isclose(test_points[\"q5\"], atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f6906bebf8284c90bd4d6aef7eb61d1",
     "grade": false,
     "grade_id": "cell-4bed7e7e23e4f5f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Put it all together\n",
    "\n",
    "That's it! We've written a little function to perform coordinate ascent using your `_update_*` functions. It tracks the log likelihood at each iteration. It also uses some nice progress bars so you can see how fast (or slow?) your code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccac2561bc497137a6705636bfbfee54",
     "grade": false,
     "grade_id": "cell-f5860cb71f1d844a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def map_estimate(templates, \n",
    "                 amplitudes, \n",
    "                 data,\n",
    "                 num_iters=20, \n",
    "                 template_rank=1,\n",
    "                 noise_std=1.0, \n",
    "                 amp_rate=5.0,\n",
    "                 tol=1e-4):\n",
    "    \"\"\"Fit the templates and amplitudes by maximum a posteriori (MAP) estimation\n",
    "    \"\"\"\n",
    "    K, N, D = templates.shape\n",
    "\n",
    "    # Make fancy reusable progress bars\n",
    "    outer_pbar = trange(num_iters)\n",
    "    inner_pbar = trange(K)\n",
    "    inner_pbar.set_description(\"updating neurons\")\n",
    "\n",
    "    # Track log likelihoods over iterations\n",
    "    lls = [log_likelihood(templates, amplitudes, data, noise_std=noise_std)]\n",
    "    for itr in outer_pbar:\n",
    "        inner_pbar.reset()\n",
    "        for k in range(K):\n",
    "            # Update the amplitude\n",
    "            amplitudes[k] = _update_amplitude(\n",
    "                k, templates, amplitudes, data, \n",
    "                noise_std=noise_std, amp_rate=amp_rate)    \n",
    "            # Update the template\n",
    "            templates[k] = _update_template(\n",
    "                k, templates, amplitudes, data, template_rank=template_rank)\n",
    "            inner_pbar.update()\n",
    "\n",
    "        # Compute the log likelihood \n",
    "        lls.append(log_likelihood(templates, amplitudes, data, \n",
    "                                  noise_std=noise_std))\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(lls[-1] - lls[-2]) < tol:\n",
    "            print(\"Convergence detected!\")\n",
    "            break\n",
    "    \n",
    "    return torch.stack(lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e376b66b9f41df1b6f7388600adf7867",
     "grade": false,
     "grade_id": "cell-cc77d45a68b4d133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fit the synthetic data and plot the log likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e6c956a59c2d0e9e7553f3e21967815",
     "grade": false,
     "grade_id": "cell-ab8fa6a71646ed28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make random templates and set amplitude to zero\n",
    "torch.manual_seed(0)\n",
    "templates = generate_templates(N, D, K)\n",
    "amplitudes = torch.zeros((K, T), device=device)\n",
    "noise_std = 1.0     # \\sigma\n",
    "amp_rate = 5.0      # \\lambda\n",
    "\n",
    "# Copy to the device\n",
    "true_templates = true_templates.to(device)\n",
    "true_amplitudes = true_amplitudes.to(device)\n",
    "templates = templates.to(device)\n",
    "amplitudes = amplitudes.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Fit the model.\n",
    "lls = map_estimate(templates, amplitudes, data, \n",
    "                   noise_std=noise_std, \n",
    "                   amp_rate=amp_rate)\n",
    "\n",
    "# For comparison, compute the log likelihood with the true templates \n",
    "# and amplitudes.\n",
    "true_ll = log_likelihood(true_templates, true_amplitudes, data, noise_std)\n",
    "\n",
    "# Plot the log likelihoods\n",
    "lls = lls.to(\"cpu\")\n",
    "true_ll = true_ll.to(\"cpu\")\n",
    "\n",
    "plt.plot(lls, '-o')\n",
    "plt.hlines(true_ll, 0, len(lls) - 1, \n",
    "           colors='r', linestyles=':', label=\"true LL\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.xlim(-.1, len(lls) - .9)\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2535948acbeb1a11872368d475779e1d",
     "grade": false,
     "grade_id": "cell-5057db10bfeca577",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Find a permutation of the inferred neurons that best matches the true neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34b0532980cef358420660d95a685208",
     "grade": false,
     "grade_id": "cell-60d37a2c51c795c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the similarity (inner product) of the true and inferred templates\n",
    "similarity = torch.zeros((K, K))\n",
    "for i in range(K):\n",
    "    for j in range(K):\n",
    "        similarity[i, j] = torch.sum(true_templates[i] * templates[j])\n",
    "        \n",
    "# Show the similarity matrix\n",
    "_, perm = linear_sum_assignment(similarity, maximize=True)\n",
    "plt.imshow(similarity[:, perm], vmin=0, vmax=1)\n",
    "plt.xlabel(\"true neuron\")\n",
    "plt.ylabel(\"inferred neuron\")\n",
    "plt.title(\"similarity of amplitudes\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3399a5b3e47e7bae369e6fd5b9877595",
     "grade": false,
     "grade_id": "cell-1482f06b5867128c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Plot the true and inferred templates\n",
    "\n",
    "They should line up pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36ad7bd81807cb37d40fbafbe28b33d3",
     "grade": false,
     "grade_id": "cell-8b33314366dcc1d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the true and inferred templates, permuted to best match\n",
    "fig, axs = plot_templates(true_templates.to(\"cpu\"), torch.arange(K), n_cols=K)\n",
    "_ = plot_templates(templates[perm].to(\"cpu\"), \n",
    "                   torch.arange(K), \n",
    "                   n_cols=K, \n",
    "                   colors=('r',), \n",
    "                   fig=fig, \n",
    "                   axs=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
