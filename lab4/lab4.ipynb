{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QAFt1ZyzFC9Z",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "824369d0af0a7005366d98089cb981e5",
     "grade": false,
     "grade_id": "cell-c512a7fae755cff2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 4: Hidden Markov Models (HMMs)\n",
    "\n",
    "<center><img src=\"https://github.com/ZKBig/8803_GMM_Lab3/blob/main/hmm_diagram.png?raw=true\" width=\"430\" height=\"200\"/></center>\n",
    "<!-- ![CA1_data](https://drive.google.com/uc?id=1y3QhKffqte7DbZmmXJFt2VC4zjbwrLGD) -->\n",
    "<center> Figure 1: Graphical model for a hidden Markov model. The hidden state variables $ z_1, \\ldots, z_N $ follow a Markov process, while each observation $ x_n $ is conditionally independent from other data given its associated hidden state variable $ z_n $.\n",
    ". </center>\n",
    "\n",
    "In this lab, we will implement forward-backward algorithm for HMMs and autoregressive HMMs to analyze the dataset concerning [depth videos of freely behaving mice](https://www.sciencedirect.com/science/article/pii/S0896627315010375?ref=pdf_download&fr=RR-2&rr=8cf1e56af9d6bfdd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1728592124297,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "Tj9KT-xcF5pG"
   },
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1728592126041,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "OeAGM6ajF7or",
    "outputId": "20d7eb23-c9d0-479f-c8d3-6221e8546a9f"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import sys\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    sys.path.append('/content/gdrive/MyDrive/Lab4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17921,
     "status": "ok",
     "timestamp": 1728592144227,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "U89pyMKrFhHj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pynwb\n",
    "!pip install dynamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 20860,
     "status": "ok",
     "timestamp": 1728592165084,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "wDzh7A_fFC9b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57164fbd6be334265d5016ab606faccf",
     "grade": false,
     "grade_id": "cell-6f2ee77717ecdb5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpynwb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NWBHDF5IO\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m play, plot_data_and_states, nearest_positive_definite, sticky_transitions, random_args, initialize_posteriors, plot_average_pcs, make_crowd_movie\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n",
      "File \u001b[0;32m~/Desktop/GT/ML-Neural-Data/lab4/utils.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m animation\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# First, import necessary libraries.\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "from tqdm.auto import trange\n",
    "from pynwb import NWBHDF5IO\n",
    "from utils import play, plot_data_and_states, nearest_positive_definite, sticky_transitions, random_args, initialize_posteriors, plot_average_pcs, make_crowd_movie\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from torch.distributions import MultivariateNormal, Normal\n",
    "\n",
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float32\n",
    "\n",
    "# Helper functions to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QvCO52WXryEr",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd2d6485e84f86a60e81ab78ad090b00",
     "grade": false,
     "grade_id": "cell-2cd410318411705b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction to the Dataset\n",
    "\n",
    "This 3D pose dynamics of mice ([The depth videos of freely behaving mice](https://www.sciencedirect.com/science/article/pii/S0896627315010375?ref=pdf_download&fr=RR-2&rr=8cf1e56af9d6bfdd)) were recorded using a single depth camera positioned above the arena. Following image extraction, corrections were applied to address parallax-induced artifacts, and the mouse was subsequently centered and aligned along its inferred spinal axis. This preprocessing enabled a detailed quantification of pose changes over time. The mouse imaged in the circular open field with a standard RGB (left) and 2D depth camera (right) is shown in Figure 2.\n",
    "\n",
    "<center><img src=\"https://github.com/ZKBig/8803_GMM_Lab3/blob/main/scenario.png?raw=true\" width=\"430\" height=\"200\"/></center>\n",
    "<!-- ![CA1_data](https://drive.google.com/uc?id=1y3QhKffqte7DbZmmXJFt2VC4zjbwrLGD) -->\n",
    "<center> Figure 2: Mouse imaged in the circular open field with a standard RGB (left) and 3D depth camera (right, mm = mm above floor). Arrow indicates the inferred axis of the animal’s spine.\n",
    ". </center>\n",
    "\n",
    "Plotting these 3D data over time showed that mouse behavior consists of periods with slowly evolving pose dynamics, interrupted by rapid transitions that separate these intervals. This pattern seems to segment the behavioral imaging data into blocks usually lasting between 200 and 900 ms. For example, this temporal structure is evident in the raw imaging pixels (Figure 3, top) and the inferred shape of the mouse’s spine (Figure 3, bottom).\n",
    "\n",
    "<center><img src=\"https://github.com/ZKBig/8803_GMM_Lab3/blob/main/observation1.png?raw=true\" width=\"4600\" height=\"200\"/></center>\n",
    "<!-- ![CA1_data](https://drive.google.com/uc?id=1y3QhKffqte7DbZmmXJFt2VC4zjbwrLGD) -->\n",
    "<center> Figure 3: Raw pixels of the extracted and aligned 3D mouse image (top panel, sorted by mean height), and height at each inferred position of the mouse’s spine (bottom panel, \"spine\" data extracted from the mouse as indicated on the right, mm = mm above floor) each reveal sporadic, sharp transitions in the pose data over time.\n",
    ". </center>\n",
    "\n",
    "The temporal structure observed in the pose dynamics data suggests a timescale at which continuous behavior can be naturally segmented into meaningful components. Visual inspection of 3D movies indicates that each sub-second block of behavior corresponds to a distinct action (e.g., a dart, a pause, the initial phase of a rear). To investigate whether these sub-second actions are stereotyped, the 3D imaging data of the mouse was processed using wavelet decomposition followed by principal component analysis (PCA). This approach transformed each block of pose dynamics into a continuous trajectory within principal component (PC) space.\n",
    "\n",
    "Now that we have covered the basic information about this 3D pose dynamics dataset of mice, let's deepen our understanding by working with it directly. Moreover, please feel free to refer to the paper [Mapping Sub-Second Structure in Mouse Behavior](https://www.sciencedirect.com/science/article/pii/S0896627315010375?ref=pdf_download&fr=RR-2&rr=8cf1e56af9d6bfdd) for more detailed preprocessing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VTaCf-lnFC9c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69bea0560c94569ac532db2a9a70593c",
     "grade": false,
     "grade_id": "cell-64ffa11ac5aba63f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Neurodata Without Borders (NWB) format\n",
    "\n",
    "NWB files are a structured form of HDF5 files. Like HDF5, NWBs are organized hierarchically into Groups and Datasets. However, NWBs differ in that they enforce a strict organization and labeling scheme to ensure uniformity across different labs.\n",
    "\n",
    "The primary organizational components of NWBs are acquisitions and processing modules. Acquisitions represent raw data streams or references to them, while processing modules consist of analyzed data derived from these acquisitions.\n",
    "\n",
    "MoSeq acquisitions contain complex and noisy Kinect video recordings that undergo processing to generate cleaned, egocentrically aligned frames, which are what you'll be working with. Since you don't need to handle preprocessing, the acquisition folder is empty, and all relevant data will be found in the MoSeq processing module.\n",
    "\n",
    "Within processing modules, there are collections called “BehavioralTimeSeries,” which contain related behavioral data time series. Inside each “BehavioralTimeSeries,” you will find “TimeSeries,” which are individual datasets representing different time series.\n",
    "\n",
    "The processing module is organized as such:\n",
    "\n",
    "- MoSeq Processing Module (top level for all MoSeq processed data)\n",
    "  - MoSeq Scalar Time Series (Behavioral time series dictionary for all MoSeq derived scalar time series)\n",
    "    - angle\n",
    "    - area\n",
    "    - etc.\n",
    "  - MoSeq Image Series (Behavioral time series dictionary for all MoSeq derived image time series)\n",
    "    - frames\n",
    "    - masks\n",
    "  - MoSeq PC Series (Behavioral time series dictionary for all MoSeq derived PC time series)\n",
    "    - principal components (with nans inserted for dropped frames)\n",
    "    - principal components cleaned (with no nans)\n",
    "  - MoSeq Label Series (Behavioral time series dictionary for all MoSeq derived lables)\n",
    "    - labels (see above about dropped frames)\n",
    "    - labels cleaned (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hVxmOt0tFC9d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5164bd8150b0374797b0934e6dceb89f",
     "grade": false,
     "grade_id": "cell-196d747ca0e7f5a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Load and Read in an NWB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://www.dropbox.com/s/564wzasu1w7iogh/moseq_data.zip\n",
    "!unzip -n moseq_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12482,
     "status": "ok",
     "timestamp": 1728592182270,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "7jj9ADkwFC9d",
    "outputId": "ed9d4edb-f40c-4c10-d270-8d234c36d264"
   },
   "outputs": [],
   "source": [
    "nwb_path = 'saline_example_0.nwb'\n",
    "io = NWBHDF5IO(nwb_path, mode='r')\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1728589835751,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "1PSCiXd2FC9d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18d2612f8bf8cf8795eb9ed195d53873",
     "grade": false,
     "grade_id": "cell-927fbc6fc797b79a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d74513c0-0dee-4cec-c403-e691e9d575d4"
   },
   "outputs": [],
   "source": [
    "# Print the contents of the processing module\n",
    "nwbfile.processing['MoSeq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1728589836467,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "1SBJ3xyRFC9d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9edda3b8c696dca567f1aeb8038278ee",
     "grade": false,
     "grade_id": "cell-3a38bb890fe66b02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "89488f4c-db5a-4641-eeca-3d736c18540e"
   },
   "outputs": [],
   "source": [
    "# Print the contents of one BehavioralTimeSeries\n",
    "nwbfile.processing['MoSeq']['Images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1728591423544,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "1-Iy198WFC9e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74d03ef1cee409b4abb2ee9ec052562a",
     "grade": false,
     "grade_id": "cell-20bf11fdeabba2b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e8fde48d-30ea-4866-e42e-021ea48d9b7a"
   },
   "outputs": [],
   "source": [
    "# Examine the 'frames' time series\n",
    "# One thing you'll notice is that the \"timestamps\" is actually another time series\n",
    "# This is to save storage space when many time series share the same time stamps\n",
    "nwbfile.processing['MoSeq']['Images']['frames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FOdsZN_1FC9e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d942bf62c8eba744cb540af6b16985d4",
     "grade": false,
     "grade_id": "cell-499ad7765863425c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Load the video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36735,
     "status": "ok",
     "timestamp": 1728529078875,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "ixgSYSLWFC9e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc4482f1c71d81a566a6c6ed2000111a",
     "grade": false,
     "grade_id": "cell-7c360e2fbbbb1abf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ff1dd0ce-1ac9-49a8-b9e8-34f423a5a961"
   },
   "outputs": [],
   "source": [
    "# Since NWBs are backed with HDF5, you can do typical lazy/partial loading here\n",
    "# Like hdf5s, you'll need to slice with [:] to get the array.\n",
    "frames = nwbfile.processing['MoSeq']['Images']['frames'].data[:]\n",
    "# Make sure to close the file when you're done!\n",
    "io.close()\n",
    "# Play a movie of the first 30 sec of data\n",
    "play(frames[:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5vGF65SPFC9e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c948e4329957a6d647ce67dbe839df7a",
     "grade": false,
     "grade_id": "cell-68c2dfeb7e781690",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.4 Load all the mice into memory\n",
    "\n",
    "The frames, even after cropping, are still 80x80 pixels. That’s a 3600 dimensional observation. In practice, the frames can be adequately reconstructed with far fewer principal components. As little as ten PCs does a pretty good job of capturing the mouse’s posture.\n",
    "\n",
    "The Datta lab has already computed the principal components and included them in the NWB. We’ll extract them, along with other relevant information like the centroid position and heading angle of the mouse, which we’ll use for making “crowd” movies below. Finally, they also included labels from MoSeq, an autoregressive (AR) HMM. You'll also build an Gaussian HMM in Part 3 of the lab and infer similar discrete latent state sequences yourself!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 54260,
     "status": "ok",
     "timestamp": 1728592236529,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "cM94VOxTFC9e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86ab4862128fc942f880d0ba384b9557",
     "grade": false,
     "grade_id": "cell-da74dee2cf41cf8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d6b5b5bb-d621-4ad9-c920-9142119b5180"
   },
   "outputs": [],
   "source": [
    "def load_dataset(indices=None,\n",
    "                 load_frames=True,\n",
    "                 num_pcs=10):\n",
    "    if indices is None:\n",
    "        indices = np.arange(24)\n",
    "\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    for t in trange(len(indices)):\n",
    "        i = indices[t]\n",
    "        nwb_path = \"saline_example_{}.nwb\".format(i)\n",
    "        with NWBHDF5IO(nwb_path, mode='r') as io:\n",
    "            f = io.read()\n",
    "            num_frames = len(f.processing['MoSeq']['PCs']['pcs_clean'].data)\n",
    "            train_slc = slice(0, int(0.8 * num_frames))\n",
    "            test_slc = slice(int(0.8 * num_frames)+1, -1)\n",
    "\n",
    "            train_data, test_data = dict(), dict()\n",
    "            for slc, data in zip([train_slc, test_slc], [train_data, test_data]):\n",
    "                data[\"raw_pcs\"] = to_t(f.processing['MoSeq']['PCs']['pcs_clean'].data[slc][:, :num_pcs])\n",
    "                data[\"times\"] = to_t(f.processing['MoSeq']['PCs']['pcs_clean'].timestamps[slc][:])\n",
    "                data[\"centroid_x_px\"] = to_t(f.processing['MoSeq']['Scalars']['centroid_x_px'].data[slc][:])\n",
    "                data[\"centroid_y_px\"] = to_t(f.processing['MoSeq']['Scalars']['centroid_y_px'].data[slc][:])\n",
    "                data[\"angles\"] = to_t(f.processing['MoSeq']['Scalars']['angle'].data[slc][:])\n",
    "                data[\"labels\"] = to_t(f.processing['MoSeq']['Labels']['labels_clean'].data[slc][:])\n",
    "\n",
    "            # only load the frames on the test data\n",
    "            # test_data[\"frames\"] = to_t(f.processing['MoSeq']['Images']['frames'].data[test_slc])\n",
    "            test_data[\"frames\"] = torch.tensor(f.processing['MoSeq']['Images']['frames'].data[test_slc])\n",
    "\n",
    "        train_dataset.append(train_data)\n",
    "        test_dataset.append(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Load a third of the dataset (8 mice)\n",
    "fps = 30\n",
    "data_dim = 10\n",
    "indices = np.arange(8)\n",
    "train_dataset, test_dataset = load_dataset(num_pcs=data_dim, indices=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rfHXpzfPFC9f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4417d76e1add7c0490e42e1f6255d3b",
     "grade": false,
     "grade_id": "cell-167a617c66bae38a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 Standardize the principal components\n",
    "\n",
    "Standardize the principal components so that they are mean zero and unit variance. This will not affect the subsequent modeling, but it will make it easier for us to visualize the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1728592237668,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "bxo36RTLFC9f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2342a840b1422b2d4d1cb07be9137a54",
     "grade": false,
     "grade_id": "cell-cdf41880af5cf692",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def standardize_pcs(dataset, mean=None, std=None):\n",
    "    if mean is None and std is None:\n",
    "        all_pcs = torch.vstack([data['raw_pcs'] for data in dataset])\n",
    "        mean = all_pcs.mean(axis=0)\n",
    "        std = all_pcs.std(axis=0)\n",
    "\n",
    "    for data in dataset:\n",
    "        data['data'] = (data['raw_pcs'] - mean) / std\n",
    "    return dataset, mean, std\n",
    "\n",
    "train_dataset, mean, std = standardize_pcs(train_dataset)\n",
    "test_dataset, _, _ = standardize_pcs(test_dataset, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1188,
     "status": "ok",
     "timestamp": 1728592239070,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "ebnuQPfYFC9f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a23c5f9855ca93fe3c4e4c19585dc789",
     "grade": false,
     "grade_id": "cell-ca11ff41cf1891d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "89f5a34c-c8cb-4a66-8e35-bbb25a577653"
   },
   "outputs": [],
   "source": [
    "plot_data_and_states(train_dataset[0], train_dataset[0][\"labels\"],\n",
    "                     title=\"data and given discrete states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ndQe7fS_FC9f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4bd6668aaf6a4926e73ac8504fc10f3",
     "grade": false,
     "grade_id": "cell-b6b94475bb004a5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should now have a `train_dataset` and a `test_dataset` loaded in memory. Each dataset is a list of dictionaries, one for each mouse. Each dictionary contains a few keys, most important of which is the data key, containing the standardized principal component time series, as shown above. For the test dataset, we also included the frames key, which has the original 80x80 images. We’ll use these to create the movies of each inferred state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4yU5vfBgFC9f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06a8c8ca1307e373aa5e576b162fcd96",
     "grade": false,
     "grade_id": "cell-34702a28d3400665",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Forward-Backward Algorithm for HMMs\n",
    "<!-- First, implement the forward-backward algorithm for computing the posterior distribution on latent states of a hidden Markov model, $ q(\\mathbf{z}) = p(\\mathbf{z} \\mid \\mathbf{x}, \\Theta) $. Specifically, this algorithm will return a $ T \\times K $ matrix where each entry represents the posterior probability that $ q(z_t = k) $. -->\n",
    "The forward-backward algorithm is a key component in inference for Hidden Markov Models (HMMs). It is used to compute the posterior distribution over the latent (hidden) states given a sequence of observed data. This process allows us to answer the question: \"What is the probability of the hidden state at a particular time given all the observed data?\"\n",
    "\n",
    "***(1) Objective***\n",
    "\n",
    "Given an HMM, our goal is to compute the posterior distribution over the latent states,\n",
    "$ q(\\mathbf{z}) = p(\\mathbf{z} \\mid \\mathbf{x}, \\Theta) $,\n",
    "where:\n",
    "- $ \\mathbf{z} = \\{z_1, z_2, \\ldots, z_T\\} $ are the hidden state variables over time $ T $,\n",
    "- $ \\mathbf{x} = \\{x_1, x_2, \\ldots, x_T\\} $ are the observed data over time $ T $,\n",
    "- $ \\Theta $ represents the parameters of the model, including the transition probabilities between states and the emission probabilities of the observations.\n",
    "\n",
    "The forward-backward algorithm leverages the structure of HMMs to efficiently compute the posterior distribution. Specifically, it returns a $ T \\times K $ matrix, where:\n",
    "- $ T $ is the number of time steps in the sequence.\n",
    "- $ K $ is the number of possible hidden states.\n",
    "\n",
    "Each entry of this matrix, $ q(z_t = k) $, represents the posterior probability that the hidden state at time $ t $ is $ k $, given all the observed data $ \\mathbf{x} $, which is $p(z_t = k | \\mathbf{x})$.\n",
    "\n",
    "***(2) Forward and Backward Passes***\n",
    "\n",
    "The forward-backward algorithm works by breaking down the problem into two main passes:\n",
    "\n",
    "1. **The Forward Pass:**\n",
    "   - The forward pass computes the probability of the observed sequence up to time $ t $ and the hidden state at time $ t $,\n",
    "\n",
    "     $ \\alpha_t(k) = p(x_1, x_2, \\ldots, x_t, z_t = k \\mid \\Theta) $.\n",
    "   - It is calculated iteratively starting from $ t = 1 $ to $ T $, using the initial state distribution and transition probabilities.\n",
    "   - This helps in computing the likelihood of being in a particular hidden state at a specific time step based on observations up to that point.\n",
    "\n",
    "2. **The Backward Pass:**\n",
    "   - The backward pass computes the probability of observing the sequence from time $ t+1 $ to $ T $, given the hidden state at time $ t $,\n",
    "\n",
    "     $ \\beta_t(k) = p(x_{t+1}, x_{t+2}, \\ldots, x_T \\mid z_t = k, \\Theta) $.\n",
    "   - It is calculated iteratively from $ t = T $ down to $ t = 1 $, and provides the likelihood of the remaining observations given a particular hidden state.\n",
    "\n",
    "***(3) Computing the Posterior Distribution***\n",
    "\n",
    "The posterior probability of being in state $ k $ at time $ t $, $ q(z_t = k) $, is computed as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(z_t = k) = \\frac{\\alpha_t(k) \\beta_t(k)}{\\sum_{k'} \\alpha_t(k') \\beta_t(k')},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "which combines the results from both the forward and backward passes.\n",
    "\n",
    "***(4) Steps to Implement the Forward-Backward Algorithm***\n",
    "\n",
    "To implement the forward-backward algorithm, follow these steps:\n",
    "\n",
    "(a). **Initialization:**\n",
    "   - Define the model parameters $ \\Theta $: the initial state probabilities, transition matrix between states, and emission probabilities of observations given states.\n",
    "   - Initialize the forward and backward matrices, $ \\alpha $ and $ \\beta $, with dimensions $ T \\times K $.\n",
    "\n",
    "(b). **Forward Pass Iteration:**\n",
    "   - For $ t = 1 $, initialize $ \\alpha_1(k) $ using the initial state distribution and the emission probabilities.\n",
    "   - For each subsequent time step $ t $, update $ \\alpha_t(k) $ based on the transition probabilities and the emission probabilities.\n",
    "\n",
    "(c). **Backward Pass Iteration:**\n",
    "   - Initialize $ \\beta_T(k) = 1 $ for all states $ k $.\n",
    "   - For each preceding time step $ t $, update $ \\beta_t(k) $ based on the transition probabilities and the emission probabilities.\n",
    "\n",
    "(d). **Compute Posterior Probabilities:**\n",
    "   - Calculate $ q(z_t = k) $ for all $ t $ and $ k $ using the equation above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lbajRm_cFC9f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "790a2ba143eebd75a96ddf4bf969f616",
     "grade": false,
     "grade_id": "cell-34607a6efa2de6a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Implement the forward pass (3 Points)\n",
    "`Notes`:\n",
    "- This function takes in the _log_ likelihoods, $\\log \\ell_{tk}$, so you'll have to exponentiate in the forward pass;\n",
    "- You need to be careful exponentiating though. If the log likelihoods are very negative, they'll all be essentially zero when exponentiated and you'll run into a divide-by-zero error when you compute the forward message $\\alpha_t$. Alternatively, if they're large positive numbers, your exponent will blow up and you'll get nan's in your calculations;\n",
    "- To avoid numerical issues, subtract $\\max_k (\\log \\ell_{tk})$ prior to exponentiating. It won't affect the forward messages, but you will have to account for it in your computation of the marginal likelihood.\n",
    "- Also note to normalize the forward messages  $\\alpha_t$ to avoid potential numerical issues. As the forward algorithm proceeds, $ \\alpha_t(k) $ values are repeatedly multiplied by probabilities (transition and observation probabilities), many of which are less than 1. This causes $ \\alpha_t(k) $ values to decrease exponentially over time. For long sequences, these values can become extremely small, potentially approaching zero. This phenomenon is known as *underflow*. We need to avoid this situation via normalizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1728592242802,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "9V8DXu3cXoMj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72c5ae3d288c46fb6bc1289453238223",
     "grade": false,
     "grade_id": "cell-1e925148136a50fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize(u, axis=0, eps=1e-15):\n",
    "    \"\"\"Normalizes the values within the axis in a way that they sum up to 1.\n",
    "\n",
    "    Args:\n",
    "        u: Input tensor to normalize.\n",
    "        axis: Axis over which to normalize.\n",
    "        eps: Minimum value threshold for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the normalized tensor, and the normalizing denominator.\n",
    "    \"\"\"\n",
    "    u = torch.where(u == 0, torch.tensor(0.0, device=u.device),\n",
    "                    torch.where(u < eps, torch.tensor(eps, device=u.device), u))\n",
    "    c = u.sum(dim=axis)\n",
    "    c = torch.where(c == 0, torch.tensor(1.0, device=u.device), c)\n",
    "    u_normalized = u / c.unsqueeze(axis)\n",
    "    return u_normalized, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1728592243597,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "BD5KZGrKFC9f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52b54e2053a2fec5045813df6a961878",
     "grade": false,
     "grade_id": "cell-c8545fe151c11ebc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(initial_dist, transition_matrix, log_likes):\n",
    "    \"\"\"\n",
    "    Perform the (normalized) forward pass of the HMM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_dist: π, the initial state distribution. Length K, sums to 1.\n",
    "    transition_matrix: P, a KxK transition matrix. Rows sum to 1.\n",
    "    log_likes: log ℓ_{t,k}, a TxK matrix of _log_ likelihoods.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alphas: TxK matrix with _normalized_ forward messages α̃_{t,k}\n",
    "    marginal_ll: Scalar marginal log likelihood log p(x | Θ)\n",
    "    \"\"\"\n",
    "    T, K = log_likes.shape                  # get the number of time steps and states\n",
    "    alphas = torch.zeros_like(log_likes)    # store normalized forward messages\n",
    "    marginal_ll = 0.0\n",
    "\n",
    "    # Step 1: get the likelihood of observation at the first itme step\n",
    "    log_ell_t = log_likes[0]\n",
    "    max_log_ell_t = torch.max(log_ell_t)\n",
    "    log_ell_t_centered = log_ell_t - max_log_ell_t\n",
    "    ell_t = torch.exp(log_ell_t_centered)\n",
    "\n",
    "    # Step 2: get the normaized alpha values for the first time step\n",
    "    alpha_t_unnormalized = ell_t * initial_dist\n",
    "    A_t = torch.sum(alpha_t_unnormalized)\n",
    "    alpha_t, _ = normalize(alpha_t_unnormalized)\n",
    "    alphas[0] = alpha_t\n",
    "    marginal_ll += torch.log(A_t) + max_log_ell_t\n",
    "\n",
    "    # Step 3: Iterate over time steps t = 1 to T-1\n",
    "    for t in range(1, T):\n",
    "        # Step 3.1: get the likelihood of observation at the current time step\n",
    "        # No more than 4 lines, similar as step 1\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        alpha_prev = alphas[t - 1]\n",
    "\n",
    "        # Step 3.2: compute the intermediate forward probabilities for the current time step\n",
    "        # by aggregating the contributions from all possible previous state\n",
    "        # no more than 1 line, using torch.matmul(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Step 3.3: get the normaized alpha values for the current time step\n",
    "        # no more than 5 lines, similar as step 2\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return alphas, marginal_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 8299,
     "status": "ok",
     "timestamp": 1728592252458,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "sqNA0KC0FC9f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3834ddd2fb234cc1aa5d4cf868a81dd1",
     "grade": true,
     "grade_id": "cell-6166cf78da9d2ea5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "b7f4b741-3e6e-4cd9-a6b5-343992d483f2"
   },
   "outputs": [],
   "source": [
    "# test your implemented forward pass\n",
    "def test_forward_pass(num_timesteps=100, num_states=10, offset=0):\n",
    "    pi, P, log_likes = random_args(num_timesteps, num_states, offset=offset)\n",
    "\n",
    "    # Call your code\n",
    "    alphas, ll = forward_pass(pi, P, log_likes)\n",
    "    assert torch.all(torch.isfinite(alphas))\n",
    "    assert torch.allclose(alphas.sum(axis=1), torch.tensor(1.0))\n",
    "\n",
    "    # Compare to Dynamax implementation.\n",
    "    from dynamax.hidden_markov_model import hmm_filter\n",
    "    pi_np = from_t(pi)\n",
    "    P_np = from_t(P)\n",
    "    log_likes_np = from_t(log_likes)\n",
    "\n",
    "    post = hmm_filter(pi_np, P_np, log_likes_np)\n",
    "    assert np.allclose(from_t(ll), post.marginal_loglik)\n",
    "    print(\"pass\")\n",
    "\n",
    "test_forward_pass()\n",
    "test_forward_pass(num_timesteps=10000, num_states=50, offset=-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fUO39gG0FC9f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "256c4d3658a93cedf55f6342d68e869a",
     "grade": false,
     "grade_id": "cell-09da525162760508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Implement the backward pass (2 Points)\n",
    "\n",
    "Recursively compute the backward messages $\\beta_t$. Again, remember to normalize the backward messages $\\beta_t$ to avoid numerical issue, and be careful when you exponentiate the log likelihoods. The same trick of subtracting the max before exponentiating will work here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728592252459,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "-eh8OVMxFC9g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa726e19687a0b560375c7c8ab9c9256",
     "grade": false,
     "grade_id": "cell-cbfa66e9ef36f33c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(transition_matrix, log_likes):\n",
    "    \"\"\"\n",
    "    Perform the (normalized) backward pass of the HMM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transition_matrix: P, a KxK transition matrix. Rows sum to 1.\n",
    "    log_likes: log ℓ_{t,k}, a TxK matrix of _log_ likelihoods.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    betas: TxK matrix with _normalized_ backward messages β̃_{t,k}\n",
    "    \"\"\"\n",
    "    T, K = log_likes.shape                            # get the number of time steps and states\n",
    "    betas = torch.zeros_like(log_likes)               # store the backward messages\n",
    "\n",
    "    # Step 1: initialization\n",
    "    backward_pred_probs = torch.ones(K, device=log_likes.device)\n",
    "    backward_pred_probs = backward_pred_probs / backward_pred_probs.sum()\n",
    "\n",
    "    # Step 2: Iterate over time steps t = T to 1\n",
    "    for t in reversed(range(T)):\n",
    "        # Step 2.1: store the current backward message\n",
    "        betas[t] = backward_pred_probs\n",
    "\n",
    "        # Step 2.2: Condition on emission at time t, being careful not to overflow.\n",
    "        # no more than 4 lines, using normalize(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Step 2.3: Predict the next state, being careful not to overflow.\n",
    "        # no more than 2 lines, using torch.matmul(...) and normalize(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return betas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Fjib7zOMFC9g",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d608a69330cf1879949b1e311d7d3a18",
     "grade": false,
     "grade_id": "cell-f8cbe2f70271a736",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Implement the E-step via combining the forward-backward passes (1 Point)\n",
    "Let us implement the E-step in this sub-section, where you need to compute the posterior probability $q(z_t = k)$ introduced previously. To compute them, combine the forward messages, backward messages, and the likelihoods, then normalize. Again, be careful when exponentiating the likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728592252459,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "g_pYbWx0FC9g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f1f01ddac6a3900d4deb39cfc572d79",
     "grade": false,
     "grade_id": "cell-2eb396de1490c67c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def E_step(initial_dist, transition_matrix, log_likes):\n",
    "    \"\"\"\n",
    "    Run the forward and backward passes and then combine to compute the\n",
    "    posterior probabilities q(z_t=k).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_dist: torch.Tensor\n",
    "        Initial state distribution (length K).\n",
    "    transition_matrix: torch.Tensor\n",
    "        A KxK transition matrix (rows sum to 1).\n",
    "    log_likes: torch.Tensor\n",
    "        A TxK matrix of log likelihoods.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    posterior: dict\n",
    "        Dictionary containing:\n",
    "        - posterior_prob: TxK matrix of posterior state probabilities.\n",
    "        - marginal_ll: Scalar marginal log likelihood.\n",
    "    \"\"\"\n",
    "    # Step 1: Run forward pass to compute alphas and marginal log likelihood\n",
    "    alphas, marginal_ll = forward_pass(initial_dist, transition_matrix, log_likes)\n",
    "\n",
    "    # Step 2: Run backward pass to compute betas\n",
    "    # no more than one line, using predefined backward_pass()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Step 3: Combine forward and backward messages to compute posterior probability\n",
    "    # no more than three lines, remember to do normalization\n",
    "    posterior_prob = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Package the results into a dictionary summarizing the posterior\n",
    "    posterior = dict(posterior_prob=posterior_prob,\n",
    "                     marginal_ll=marginal_ll)\n",
    "\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 5647,
     "status": "ok",
     "timestamp": 1728592258104,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "wxgFuieKFC9g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10999a9f95d01bb8a621c9b2419df8bf",
     "grade": true,
     "grade_id": "cell-4acc7ffe9db103bf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f9691a50-5ae3-4dbd-a152-6466e9b20e1d"
   },
   "outputs": [],
   "source": [
    "# test your own implementation\n",
    "def test_E_step(num_timesteps=100, num_states=10, offset=0):\n",
    "    pi, P, log_likes = random_args(num_timesteps, num_states, offset=offset)\n",
    "\n",
    "    # Run your code\n",
    "    posterior = E_step(pi, P, log_likes)\n",
    "\n",
    "    # Run Dynamax code\n",
    "    from dynamax.hidden_markov_model import hmm_two_filter_smoother\n",
    "    pi_np = from_t(pi)\n",
    "    P_np = from_t(P)\n",
    "    log_likes_np = from_t(log_likes)\n",
    "    post = hmm_two_filter_smoother(pi_np, P_np, log_likes_np)\n",
    "\n",
    "    assert np.allclose(posterior[\"posterior_prob\"], post.smoothed_probs)\n",
    "    assert np.allclose(posterior[\"marginal_ll\"], post.marginal_loglik)\n",
    "    print(\"pass\")\n",
    "\n",
    "test_E_step()\n",
    "test_E_step(num_timesteps=10000, num_states=50, offset=-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NyukyOPETlv0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b5e443b04d6903446e3b1c2cebac47b",
     "grade": false,
     "grade_id": "cell-1a00f9da24647047",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Gaussian Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mzdRbEHGy6Dd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42171b1b990aa5c50a0dcc44d9291e6f",
     "grade": false,
     "grade_id": "cell-2cace385cc4ff4e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, we will implement a Hidden Markov Model (HMM) with Gaussian observations. The model can be expressed mathematically as:\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x, z \\mid \\Theta) &= \\mathrm{Cat}(z_1 \\mid \\pi) \\prod_{t=2}^{T} \\mathrm{Cat}(z_t \\mid P_{z_{t-1}}) \\prod_{t=1}^T \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t})\n",
    "\\end{align}\n",
    "$$\n",
    "where the parameters are $\\Theta = \\{\\pi, P, \\{b_k, Q_k\\}_{k=1}^K\\}$. Here:\n",
    "- $ x_t \\in \\mathbb{R}^{D} $ represents the observed data at time $ t $,\n",
    "- $ z_t \\in \\{1, \\ldots, K\\} $ denotes the latent state at time $ t $,\n",
    "- $ \\pi $ is the initial state distribution,\n",
    "- $ P $ is the transition matrix for the latent states,\n",
    "- $ \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t}) $ represents the Gaussian distribution with mean $ b_{z_t} $ and covariance matrix $ Q_{z_t} $.\n",
    "### Maximization Step (M-step) for EM Algorithm\n",
    "To update the parameters of the Gaussian HMM using the Expectation-Maximization (EM) algorithm, we work with the Evidence Lower Bound (ELBO). Because in this lab, we will only learn the parameters in the Gaussian observations, that is $b$ and $Q$, not $\\pi$ and $P$, the ELBO in terms of $ b $ and $ Q $ is given as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q, \\Theta) = \\mathbb{E}_{q(z)} \\left[ \\sum_{t=1}^T \\mathbb{I}[z_t = k] \\cdot \\log \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t}) \\right] + c,\n",
    "\\end{align}\n",
    "$$\n",
    "Here, $\\mathbb{I}$ is the indicator function $$\\mathbb{I}[\\text{condition}] = \n",
    "\\begin{cases} \n",
    "1 & \\text{if the condition is true} \\\\\n",
    "0 & \\text{if the condition is false}\n",
    "\\end{cases}$$ and $ c $ encompasses all other terms in the ELBO that involve the categorical distributions. Explicitly:\n",
    "$$\n",
    "c = \\mathbb{E}_{q(z)} \\left[ \\log \\mathrm{Cat}(z_1 \\mid \\pi) + \\sum_{t=2}^{T} \\log \\mathrm{Cat}(z_t \\mid P_{z_{t-1}}) \\right]\n",
    "$$\n",
    "Because the above $c$ expression does not include $b$ and $Q$, we just simplify them as a constant. Then, expanding $\\mathcal{L}(q, \\Theta)$ and taking the expectation, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q, \\Theta) = \\sum_{t=1}^T \\sum_{k=1}^K q(z_t = k) \\left( -\\frac{D}{2} \\log(2\\pi) - \\frac{1}{2} \\log|Q_k| - \\frac{1}{2}(x_t - b_k)^\\top Q_k^{-1}(x_t - b_k) \\right) + c,\n",
    "\\end{align}\n",
    "$$\n",
    "where:\n",
    "- $ q(z_t = k) $ is the probability that the latent state $ z_t $ equals $ k $ at time $ t $,\n",
    "- $ D $ is the dimensionality of the observation data $ x $.\n",
    "To find the updated parameters, we take the gradients of $ \\mathcal{L}(q, \\Theta) $ with respect to $ b $ and $ Q $ and set them to zero. The updated mean and covariance expressions are:\n",
    "$$\n",
    "\\begin{align}\n",
    "b_k^{\\text{new}} = \\frac{t_{k,1}}{N_k},\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_k^{\\text{new}} = \\frac{t_{k,2}}{N_k} - b_k^{\\text{new}} {b_k^{\\text{new}}}^\\top + \\epsilon I,\n",
    "\\end{align}\n",
    "$$\n",
    "where:\n",
    "- $ N_k = \\sum_{t=1}^T q(z_t = k) $ is the total expected count of state $ k $,\n",
    "- $ t_{k,1} = \\sum_{t=1}^T q(z_t = k) x_t $ is the weighted sum of observations for state $ k $,\n",
    "- $ t_{k,2} = \\sum_{t=1}^T q(z_t = k) x_t x_t^\\top $ is the weighted sum of outer products for state $ k $,\n",
    "- $x_t$ and $ x_t x_t^\\top $ are the sufficient statistics because they contain all the information necessary to estimate the parameters $ b_k $ and $ Q_k $ of the Gaussian distributions,\n",
    "- $ \\epsilon I $ is added to ensure the positive definiteness of the covariance matrix.\n",
    "\n",
    "Since the dataset contains multiple trajectories, each representing a different mouse, we must aggregate information across all trajectories when computing $N_k$, $t_{k,1}$, $t_{k,2}$. Let $ M $ denote the number of mice in the dataset, and let $ x_t^{(m)} $ and $ z_t^{(m)} $ denote the observation and latent state at time $ t $ for mouse $ m $, respectively. The aggregated expressions are:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar{N}_k = \\sum_{m=1}^{M} \\sum_{t=1}^{T_m} q(z_t^{(m)} = k),\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar{t}_{k,1} = \\sum_{m=1}^{M} \\sum_{t=1}^{T_m} q(z_t^{(m)} = k) x_t^{(m)},\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar{t}_{k,2} = \\sum_{m=1}^{M} \\sum_{t=1}^{T_m} q(z_t^{(m)} = k) x_t^{(m)} x_t^{(m)\\top}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that $\\bar{N}_k$, $\\bar{t}_{k,1}$, $\\bar{t}_{k,2}$ are called expected sufficient statistics. We will now implement the computation of $\\bar{N}_k$, $\\bar{t}_{k,1}$, $\\bar{t}_{k,2}$ and perform the M-step for Gaussian HMMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HZn3gYQbToVE",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ade91c6b92dc101913f2a88ad85b1c1b",
     "grade": false,
     "grade_id": "cell-f7755cdbabccee7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Implement the Computation of $\\bar{N}_k$, $\\bar{t}_{k,1}$, $\\bar{t}_{k,2}$ (1.5 Points)\n",
    "\n",
    "In this sub-section, you need to implement a function to compute the tuple of $\\bar{N}_k$, $\\bar{t}_{k,1}$, $\\bar{t}_{k,2}$, then normalize them by the length of the sequence to avoid numerical issue. The above terms are combined across all mice (i.e., all the data dictionaries and posterior dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1728592259083,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "mOTdhQUpy55B",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9a558698295d07c6ca83ed9ac453f56",
     "grade": false,
     "grade_id": "cell-7f3471bd0fa662bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def precompute_suff_stats(dataset):\n",
    "    \"\"\"\n",
    "    Compute N_k, t_{k,1}, t_{k,2} of the Gaussian distribution for each\n",
    "    data dictionary in the dataset. This modifies the dataset in place.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: a list of data dictionaries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Nothing, but the dataset is updated in place to have a new `suff_stats`\n",
    "        key, which contains a tuple of N_k, t_{k,1}, t_{k,2}.\n",
    "    \"\"\"\n",
    "    for data in dataset:\n",
    "        x = data['data']\n",
    "        data['suff_stats'] = (x,                                   # x_t\n",
    "                              torch.einsum('ti,tj->tij', x, x))    # x_t x_t^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1728592260392,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "INktBZCpUjQF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "613e9a1061cc215f94260ba095733dc1",
     "grade": false,
     "grade_id": "cell-550242e058c93255",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "precompute_suff_stats(train_dataset)\n",
    "precompute_suff_stats(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1728592261141,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "74arFzCbUncd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9634fb7f0dcab420b66f4edd16ba94e",
     "grade": false,
     "grade_id": "cell-5063266d6b61385f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_suff_stats(dataset, posteriors):\n",
    "    \"\"\"\n",
    "    Compute a tuple of normalized N_k, t_{k,1}, t_{k,2} via dividing them by the\n",
    "    length of the sequence. They are combined across all mice (i.e., all the data\n",
    "    dictionaries and posterior dictionaries).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: a list of dictionaries with multiple keys, including \"data\", the TxD\n",
    "        array of observations for this mouse, and \"suff_stats\", the tuple of\n",
    "        t_{k,1}, t_{k,2}.\n",
    "        .\n",
    "    posteriors: a list of dictionaries with the key \"posterior_prob\", which is\n",
    "        a TxK array of posterior probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats: a tuple of normalized N_k, t_{k,1}, t_{k,2}.\n",
    "    \"\"\"\n",
    "    assert isinstance(dataset, list)\n",
    "    assert isinstance(posteriors, list)\n",
    "\n",
    "    # for a single time series and corresponding posterior.\n",
    "    def _compute_expected_suff_stats(data, posterior):\n",
    "        q = posterior['posterior_prob']\n",
    "        suff_stats = data['suff_stats']\n",
    "        t0 = suff_stats[0]\n",
    "        t1 = suff_stats[1]\n",
    "\n",
    "        T = t0.shape[0]  # Length of the sequence\n",
    "\n",
    "        # step1: N for each state\n",
    "        N = None\n",
    "        # no more than one line, using tensor.sum(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # step2: t1 for each state\n",
    "        t1s = None\n",
    "        # no more than one line, using torch.einsum(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # step3: t1 for each state\n",
    "        t2s = None\n",
    "        # no more than one line, using torch.einsum(...)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        stats = (N, t1s, t2s)\n",
    "        return T, stats\n",
    "\n",
    "    # Initialize combined tuple of N_k, t_{k,1}, t_{k,2}\n",
    "    combined_T = 0\n",
    "    combined_stats = None\n",
    "\n",
    "    # Sum the expected stats over the whole dataset\n",
    "    for data, posterior in zip(dataset, posteriors):\n",
    "        T, these_stats = _compute_expected_suff_stats(data, posterior)\n",
    "        if combined_stats is None:\n",
    "            combined_stats = these_stats\n",
    "        else:\n",
    "            # normalize by T\n",
    "            combined_stats = tuple((cs + ts) / T for cs, ts in zip(combined_stats, these_stats))\n",
    "\n",
    "    return combined_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "giBuG9mvYaeT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5d62371bd8d2d456b083f9ed25089cd",
     "grade": false,
     "grade_id": "cell-c61c3741ffc822c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Implement Maximization Process for Gaussian HMM (1 Point)\n",
    "\n",
    "Please update the means and covariances in terms of the previous given expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1728592262924,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "RYghrMlkYRGb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d709b6a6a66076bd1d27efa754f86078",
     "grade": false,
     "grade_id": "cell-3124b4cd5ce6d30d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def M_step(num_states, data_dim, stats):\n",
    "    \"\"\"\n",
    "    Compute the updated means and covariances of the Gaussian distribution\n",
    "    Note: add a small amount (1e-4 * I) to the diagonal of each covariance\n",
    "        matrix to ensure that the result is positive definite.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_states: a scaler indicating the number of states\n",
    "    data_dim: a scaler indicating the dimension of data\n",
    "    stats: a tuple of N_k, t_{k,1}, t_{k,2}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    updated means and covs\n",
    "    \"\"\"\n",
    "    Ns, t1s, t2s = stats\n",
    "    K = num_states\n",
    "    D = data_dim\n",
    "\n",
    "    ###\n",
    "    # Update means\n",
    "    # no more than one line\n",
    "    means = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Initialize covariance matrices\n",
    "    covs = torch.zeros(K, D, D)\n",
    "\n",
    "    # Update covariances for each state\n",
    "    for k in range(K):\n",
    "        # Ensure Ns[k] is greater than zero to avoid division by zero\n",
    "        if Ns[k] > 0:\n",
    "            # Update covariance\n",
    "            # no more than four lines, remember to use torch.outer(...) and torch.eye(...)\n",
    "            cov_k = None\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            covs[k] = cov_k\n",
    "        else:\n",
    "            # If Ns[k] equals to 0, use the identity matrix as the covariance\n",
    "            covs[k] = torch.eye(D)\n",
    "\n",
    "    return means, covs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EygyDXPBadS3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29f5bcecd7db58a033e0ff80f31b2ac3",
     "grade": false,
     "grade_id": "cell-2ab016d0826e22f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3 Fit the Gaussian HMM with EM (1.5 Points)\n",
    "\n",
    "In this section, you are going to implement a function to fit a Gaussian HMM with EM.\n",
    "\n",
    "`Note`: This is only a partial fit, as this method will treat the initial state distribution and the transition matrix as fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1728592267237,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "hPEV1zKgbyZX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d3c8ccfbe8b7bc6c15bbfa7ad1ce3a6",
     "grade": false,
     "grade_id": "cell-c9cadf05ef0ab76e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_log_likelihoods(num_states, means, covs, data):\n",
    "    \"\"\"\n",
    "    Compute the matrix of log likelihoods of data for each state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_states: a scaler indicating the number of states\n",
    "    means: a KxD array of means for each state\n",
    "    covs: a KxDxD array of covariance matrices for each state\n",
    "    data: a dictionary with multiple keys, including \"data\", the TxD array\n",
    "        of observations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_likes: a TxK array of log likelihoods for each datapoint and\n",
    "        discrete state.\n",
    "    \"\"\"\n",
    "    x = data[\"data\"]\n",
    "    T = x.shape[0]\n",
    "    K = num_states\n",
    "    log_likes = torch.zeros(T, K)\n",
    "\n",
    "    # Loop over each state to compute the log likelihoods\n",
    "    for k in range(K):\n",
    "        mvn = torch.distributions.MultivariateNormal(means[k], covs[k])\n",
    "        log_likes[:, k] = mvn.log_prob(x)\n",
    "\n",
    "    return log_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1728592268033,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "NUGDG5ARcmCq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f5c85dbde9a4021481c4b672fdaf9e2",
     "grade": false,
     "grade_id": "cell-b8340fd4d0acf36e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit(train_dataset,\n",
    "        test_dataset,\n",
    "        initial_dist,\n",
    "        transition_matrix,\n",
    "        num_states,\n",
    "        data_dim,\n",
    "        seed=0,\n",
    "        num_iters=40):\n",
    "    \"\"\"\n",
    "    Fit a Hidden Markov Model (HMM) with expectation maximization (EM).\n",
    "\n",
    "    Note: This is only a partial fit, as this method will treat the initial\n",
    "    state distribution and the transition matrix as fixed!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataset: a list of dictionaries with multiple keys, including \"targets\",\n",
    "        the TxD array of observations, \"covariates\", the TxP array of covariates,\n",
    "        and \"suff_stats\", the tuple of N_k, t_{k,1}, t_{k,2}.\n",
    "\n",
    "    test_dataset: as above but only used for tracking the test log likelihood\n",
    "        during training.\n",
    "\n",
    "    num_states: a scaler indicating the number of states.\n",
    "\n",
    "    data_dim: a scaler indicating the dimension of data\n",
    "\n",
    "    initial_dist: a length-K vector giving the initial state distribution.\n",
    "\n",
    "    transition_matrix: a K x K matrix whose rows sum to 1.\n",
    "\n",
    "    seed: random seed for initializing the algorithm.\n",
    "\n",
    "    num_iters: number of EM iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_lls: array of likelihoods of training data over EM iterations\n",
    "    test_lls: array of likelihoods of testing data over EM iterations\n",
    "    posteriors: final list of posterior distributions for the training data\n",
    "    test_posteriors: final list of posterior distributions for the test data\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    num_train = sum([len(data[\"data\"]) for data in train_dataset])\n",
    "    num_test = sum([len(data[\"data\"]) for data in test_dataset])\n",
    "\n",
    "    # Check the initial distribution and transition matrix\n",
    "    assert initial_dist.shape == (num_states,) and \\\n",
    "        torch.all(initial_dist >= 0) and \\\n",
    "        torch.isclose(initial_dist.sum(), torch.tensor(1.0))\n",
    "    assert transition_matrix.shape == (num_states, num_states) and \\\n",
    "        torch.all(transition_matrix >= 0) and \\\n",
    "        torch.allclose(transition_matrix.sum(axis=1), torch.tensor(1.0))\n",
    "\n",
    "    # Initialize posterior and N_k, t_{k,1}, t_{k,2}\n",
    "    posteriors = initialize_posteriors(train_dataset, num_states, seed=seed)\n",
    "    stats = compute_expected_suff_stats(train_dataset, posteriors)\n",
    "\n",
    "    train_lls = []\n",
    "    test_lls = []\n",
    "\n",
    "    for itr in trange(num_iters):\n",
    "        # M step: update the parameters of the emission part in ELBO\n",
    "        # one line code\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # E step: compute the posterior for each data dictionary in the dataset\n",
    "        # using for loop to iterate over all training sampels\n",
    "        # no more than 5 lines, using get_log_likelihoods, E_step\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Compute the tuple of N_k, t_{k,1}, t_{k,2} under the new posteriors\n",
    "        stats = compute_expected_suff_stats(train_dataset, posteriors)\n",
    "\n",
    "        # Store the average train likelihood\n",
    "        avg_train_ll = sum([p[\"marginal_ll\"] for p in posteriors]) / num_train\n",
    "        train_lls.append(avg_train_ll)\n",
    "\n",
    "        # Compute the posteriors for the test dataset too\n",
    "        # using for loop to iterate over all test sampels\n",
    "        # no more than 5 lines, using get_log_likelihoods, E_step\n",
    "        test_posteriors = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Store the average test likelihood\n",
    "        avg_test_ll = sum([p[\"marginal_ll\"] for p in test_posteriors]) / num_test\n",
    "        test_lls.append(avg_test_ll)\n",
    "\n",
    "    train_lls = torch.tensor(train_lls)\n",
    "    test_lls = torch.tensor(test_lls)\n",
    "    return train_lls, test_lls, posteriors, test_posteriors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 38752,
     "status": "ok",
     "timestamp": 1728592307352,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "u2Yn35JJdeCY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da6bfcf3ef58986f3288a754860f9d34",
     "grade": false,
     "grade_id": "cell-5384bdcb314e0118",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "4bbf313f-21e2-477d-f36c-72cc0aa84e31"
   },
   "outputs": [],
   "source": [
    "num_states = 50\n",
    "initial_dist = torch.ones(num_states) / num_states\n",
    "transition_matrix = sticky_transitions(num_states, stickiness=0.95)\n",
    "\n",
    "# Fit it! It will take about 11 minutes.\n",
    "train_lls, test_lls, train_posteriors, test_posteriors, = \\\n",
    "    fit(train_dataset[:1],\n",
    "            test_dataset[:1],\n",
    "            initial_dist,\n",
    "            transition_matrix,\n",
    "            num_states,\n",
    "            data_dim)\n",
    "\n",
    "plt.plot(train_lls, label=\"train\")\n",
    "plt.plot(test_lls, '-r', label=\"test\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"avg marginal log lkhd\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pNl0zQ2bTCxB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f79524858b454a7c869f856f706e087a",
     "grade": false,
     "grade_id": "cell-ca96fa3c8838a693",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.4 Plot the data and the inferred states\n",
    "\n",
    "\n",
    "We'll make the same plot as above (in the warm-up) but using our inferred states instead. Hopefully, the states seem to switch along with changes in the data.\n",
    "\n",
    "**Note**: We're showing the state with the highest marginal probability, $z_t^\\star = \\mathrm{arg} \\, \\mathrm{max}_k \\; q(z_t = k)$. This is different from the most likely state path, $z_{1:T}^\\star = \\mathrm{arg}\\,\\mathrm{max} \\; q(z)$. We could compute the latter with the Viterbi algorithm, which is similar to the forward-backward algorithm you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1273,
     "status": "ok",
     "timestamp": 1728592311974,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "pIgVwulGJZ-C",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f41000702b3b6860ec3ff316aa39f636",
     "grade": false,
     "grade_id": "cell-1b7f3532e32cde4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ebe72aa6-4761-4c31-c378-61d5ce3bdb34"
   },
   "outputs": [],
   "source": [
    "plot_data_and_states(train_dataset[0],\n",
    "                     train_posteriors[0][\"posterior_prob\"].argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XGeqrYSuTIXe",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f08f2d3da689514cb8e6629ae2c108da",
     "grade": false,
     "grade_id": "cell-1cebaac96f2fcdd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.5 Plot the state usage histogram\n",
    "\n",
    "\n",
    "The state usage histogram shows how often each discrete state was used under the posterior distribution. You'll probably see a long tail of states with non-trivial usage (hundreds of frames), all the way out to state 50. That suggests the model is using all its available capacity, and we could probably crank the number of states up even further for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1728592314475,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "yxmiasl9TEoB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d938343c0f92d45fc01b9a0c65e23aa",
     "grade": false,
     "grade_id": "cell-0c3544b84fcf78f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "291f5a96-eac4-43d3-a6a0-943ddb1ee0d5"
   },
   "outputs": [],
   "source": [
    "# Sort states by usage\n",
    "ghmm_states = train_posteriors[0][\"posterior_prob\"].argmax(1)\n",
    "ghmm_usage = torch.bincount(ghmm_states, minlength=num_states)\n",
    "ghmm_order = torch.argsort(ghmm_usage, descending=True)\n",
    "\n",
    "plt.bar(torch.arange(num_states), ghmm_usage[ghmm_order])\n",
    "plt.xlabel(\"state index [ordered]\")\n",
    "plt.ylabel(\"num frames\")\n",
    "plt.title(\"histogram of inferred state usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8o-2eeaQThAr",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee8865af4d6fdbf0c96b5b0d9e0138dd",
     "grade": false,
     "grade_id": "cell-c47616df29f94ef7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.6 Plot the average PC trajectory time locked to state entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 536,
     "status": "error",
     "timestamp": 1728592318076,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "GCTFdyZ7TJ9W",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "328aab3089fe41e38a0eee0d9c428655",
     "grade": false,
     "grade_id": "cell-80951b1f9a273a63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3d95c9b5-5f62-4cf9-cb2e-a3ed5d0b6b4e"
   },
   "outputs": [],
   "source": [
    "plot_average_pcs(ghmm_order[34], train_dataset[:1], train_posteriors[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CDE3aCR3Tl4B",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "165b8569d0d07a6eae1afbed2a4c3add",
     "grade": false,
     "grade_id": "cell-58f4097a44755668",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.7 Plot some \"crowd\" movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 16940,
     "status": "ok",
     "timestamp": 1728531595577,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "htzXerEGTTX5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c92d96ac27c5c8ef5dbfb3b5250739bb",
     "grade": false,
     "grade_id": "cell-591278c884eac6f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "74e8d3d7-0f5c-46cc-e7a7-1376b0747f62"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[0], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 11629,
     "status": "ok",
     "timestamp": 1728531613171,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "cx3F9G7gTpJu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a414e8f00d5fc5c6e3ddfd4dd269bb80",
     "grade": false,
     "grade_id": "cell-bd8b8aba63065ef0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf4b9968-d2bc-442d-a40a-9120ffc896af"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[1], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 14807,
     "status": "ok",
     "timestamp": 1728531627974,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "kyGBxT25UulS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "110d8a0913d3dbe896f4cf500ef4679e",
     "grade": false,
     "grade_id": "cell-caca52526b42a537",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a36de392-124d-4f3e-d821-fe965fda096c"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[2], test_dataset, test_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 5516,
     "status": "ok",
     "timestamp": 1728531645630,
     "user": {
      "displayName": "Victor Wang",
      "userId": "17518693013141002863"
     },
     "user_tz": 240
    },
    "id": "hME8O7YbU0kn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22e0ba51eb3f4cf930601c3f9bb5a6ad",
     "grade": false,
     "grade_id": "cell-f0f9d48a55d60821",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "30e16ccd-3c53-4b7b-bf81-4f5fb53b3d7a"
   },
   "outputs": [],
   "source": [
    "play(make_crowd_movie(ghmm_order[30], test_dataset, test_posteriors))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nbd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
