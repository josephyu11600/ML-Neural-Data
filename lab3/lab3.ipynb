{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"HA-YvDJpvUYF","nbgrader":{"checksum":"10cea7248a33ecf364b26e082c40b362","grade":false,"grade_id":"cell-7e4e286a5cf70ea8","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Lab3: Density Estimation and GMM\n","In this lab, you will practice the common density estimation methods including `MLE` and `MAP`. You will also learn to apply `Guassian Mixture Models` and `Expectation-Maximization` algorithm to the real nerual dataset and analyze the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FU-s3xIvUYH"},"outputs":[],"source":["using_colab = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5nFXoQjvUYI"},"outputs":[],"source":["if using_colab:\n","    import sys\n","    from google.colab import drive\n","    drive.mount('/content/gdrive/')\n","    sys.path.append('/content/gdrive/MyDrive/lab3/')"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"BRvunE8yvUYJ","nbgrader":{"checksum":"0de026ea36b14cbbc74f439f1a503925","grade":false,"grade_id":"cell-faf19a904ee2903f","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from scipy.io import loadmat\n","from scipy.stats import multivariate_normal\n","\n","import matplotlib.pyplot as plt\n","from torch.distributions import Poisson\n","import seaborn as sns\n","import numpy as np\n","import h5py\n","import torch"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"uh9UJyn6vUYJ","nbgrader":{"checksum":"2f61c4b16aeaab06352f15dca11c5b9d","grade":false,"grade_id":"cell-229bfb1793e8b328","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## 1. Marginal & Conditional Probability Densities (2.5pt)\n","\n","Please load the `gaussian_distribution.mat` file, which contains 2D Gaussian distribution probability density given by:\n","\n","$$\n","p(x, y) = \\frac{1}{2\\pi\\sqrt{|\\Sigma|}} \\exp\\left(-\\frac{1}{2} \\mathbf{z}^T \\Sigma^{-1} \\mathbf{z}\\right)\n","$$\n","\n","Where:\n","- $\\mathbf{z} = \\begin{pmatrix} x - \\mu_x \\\\ y - \\mu_y \\end{pmatrix}$ is the difference between the point $ (x, y) $ and the mean vector;\n","- $\\Sigma$ is the covariance matrix;\n","- $|\\Sigma|$ is the determinant of the covariance matrix;\n","- $\\Sigma^{-1}$ is the inverse of the covariance matrix.\n","\n","\n","Then, run the following code to illustrate it. Note that the variables in the loaded data defined include:\n","\n","$\\quad$ `x`: a vector of $x$ points\n","\n","$\\quad$ `y`: a vector of $y$ points\n","\n","$\\quad$ `pdf`: a 2D matrix, whose $i, j$ 'th entry is the 2D Gaussian probability\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwXqOwj5vUYJ"},"outputs":[],"source":["# read the mat format data\n","if using_colab:\n","    data = loadmat(\"/content/gdrive/MyDrive/lab3/data/gaussian_distribution.mat\")\n","else:\n","    data = loadmat('./data/gaussian_distribution.mat')\n","x = data['x']\n","y = data['y']\n","pdf = data['pdf']\n","\n","# plot the 2D Gaussian distribution\n","fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_surface(x, y, pdf, cmap='viridis', edgecolor='none')\n","ax.set_title(\"2D Gaussian Distribution\", fontsize=12, fontweight='bold')\n","ax.set_xlabel(\"X\", fontsize=12, fontweight='bold')\n","ax.set_ylabel(\"Y\", fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=12, fontweight='bold')\n","plt.yticks(fontsize=12, fontweight='bold')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Z6VmMm88vUYK","nbgrader":{"checksum":"1124481a3cc59da3fedcf1bb99870c08","grade":false,"grade_id":"cell-2bc726bb0261dd7d","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["**1)** From the above joint 2D Gaussian density, we can find the marginal distribution of X by integerating the joint distribution over all possible values of Y, which is given as:\n","\n","$$\n","p(x) = \\int_{-\\infty}^{\\infty} p(x, y) \\, dy = \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi \\sqrt{|\\Sigma|}} \\exp\\left( -\\frac{1}{2} \\mathbf{z}^T \\Sigma^{-1} \\mathbf{z} \\right) dy\n","$$\n","By solving this above formula, the marginal probability of X can be expressed as:\n","$$\n","p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left( -\\frac{(x - \\mu_x)^2}{2\\sigma_x^2} \\right)\n","$$\n","\n","Similarly, the marginal probability of Y can be solved as:\n","$$\n","p(y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left( -\\frac{(y - \\mu_y)^2}{2\\sigma_y^2} \\right)\n","$$\n","\n","In practice, when we use numerical methods, the above integral process can be approximated by summing the joint probability over a range of X or Y values.  Please complete the following codes to compute the marginal probability of X (0.5pt) and Y (0.5pt) and plot them.\n","\n","`Note`: The integral is not just summing these values; it's summing the \"area\" under the curve of $ p(x, y) $ as $ y $ varies. The area of each slice is the height of  $ p(x, y_{i}) $ multiplied by the width of the slice $ \\Delta y $."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"x65HmstRvUYK","nbgrader":{"checksum":"f7cadbe581b5581e7de9c4a7f1b633c9","grade":false,"grade_id":"cell-73b77c566e18cac7","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# P(x) - the marginal distribution over x\n","# step1: Obtain dy\n","# step2: Sum the area under the curve of 2D Gaussian distribution as y varies (np.sum)\n","P_X = None\n","# YOUR CODE HERE\n","raise NotImplementedError()\n","\n","# P(y) - the marginal distribution over y\n","# step1: Obtain dx\n","# step2: Sum the area under the curve of 2D Gaussian distribution as x varies (np.sum)\n","P_Y = None\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"64g7w1POPYII","nbgrader":{"checksum":"f3aa274f3e41eb86ae67b6b3ecdd64da","grade":true,"grade_id":"cell-c1d64dc34aa02b85","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# TEST YOUR ANSWER\n","assert P_X.shape == (600, )\n","assert P_Y.shape == (600, )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"v68bblO_vUYL","nbgrader":{"checksum":"a3a33721ef4ccd01bbf0aaee3efb4c6a","grade":false,"grade_id":"cell-a641bac1d93bd2db","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# plot the two marginal distribution\n","plt.figure(figsize=(4, 4))\n","plt.plot(x[:, 0], P_X, label='P(X)',)\n","plt.plot(y[0, :], P_Y, label='P(Y)',)\n","plt.xlabel('Value', fontsize=12, fontweight='bold')\n","plt.ylabel('Density', fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=12, fontweight='bold')\n","plt.yticks(fontsize=12, fontweight='bold')\n","plt.title('Marginal Distributions of X and Y', fontsize=12, fontweight='bold')\n","plt.legend()\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"D1vBQIoQvUYL","nbgrader":{"checksum":"3b72c75f885b48e0f68c524d560adf6c","grade":false,"grade_id":"cell-5200d008fa68e7df","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["**2)** The conditional probability can be computed using the joint probability density divided by the marginal probability density, which can be expressed as:\n","\n","$$\n","P(Y | X = x) = \\frac{P(X = x, Y = y)}{P(X = x)}\n","$$\n","\n","Please calculate and plot the conditional probability $p(x|y)$ (0.5pt) and $p(y|x)$ (0.5pt) for the 2D Gaussian distribution according to the above expression.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"KcmL-JaAvUYL","nbgrader":{"checksum":"27ac4f3f974a279eb24e22cc3a02708e","grade":false,"grade_id":"cell-aff54180287196d3","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# P(x|y) - the full image of the conditional density P(x|y)\n","P_X_Y = None\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"2ya9xxKdQbiw","nbgrader":{"checksum":"fd2fab8e696653578e78fffae4babb36","grade":true,"grade_id":"cell-af95d6e47d396fb5","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# TEST YOUR ANSWER\n","assert P_X_Y.shape == (600, 600)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Z2NkQ2IMvUYL","nbgrader":{"checksum":"cfe03941be677cfc03a7fb0866674c95","grade":false,"grade_id":"cell-d5ebbbd10385cb09","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\n","fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_surface(x, y, P_X_Y, cmap='viridis', edgecolor='none')\n","ax.set_title(\"2D Gaussian Distribution\", fontsize=12, fontweight='bold')\n","ax.set_xlabel(\"X\", fontsize=12, fontweight='bold')\n","ax.set_ylabel(\"Y\", fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=12, fontweight='bold')\n","plt.yticks(fontsize=12, fontweight='bold')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"p7PPbSVxvUYM","nbgrader":{"checksum":"6b36cff70752361fa778903579142a4c","grade":false,"grade_id":"cell-d413d508ca27efde","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# P(y|x) - the full image of the conditional density P(y|x)\n","P_Y_X = None\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"n4GL0rwdQnPv","nbgrader":{"checksum":"92a8397bf99e457a2dbdde09cf0cf772","grade":true,"grade_id":"cell-4c6ca8f2cdcb2607","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# TEST YOUR ANSWER\n","assert P_Y_X.shape == (600, 600)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"aueUNF2ovUYM","nbgrader":{"checksum":"25cfc09c6f2e231987e2903ba4398554","grade":false,"grade_id":"cell-193988be6594890a","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_surface(x, y, P_Y_X, cmap='viridis', edgecolor='none')\n","ax.set_title(\"2D Gaussian Distribution\", fontsize=12, fontweight='bold')\n","ax.set_xlabel(\"X\", fontsize=12, fontweight='bold')\n","ax.set_ylabel(\"Y\", fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=12, fontweight='bold')\n","plt.yticks(fontsize=12, fontweight='bold')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Y3zOoA1uvUYM","nbgrader":{"checksum":"904e5e3d2642763d021031c08903863a","grade":false,"grade_id":"cell-1fc041e8b0202279","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["**3)** If X and Y ar independent, knowing the value of X gives no information about Y, and vice versa. This can be expressed as the following formula:\n","\n","$$\n","P(X = x, Y = y) = P(X = x)P(Y = y)\n","$$\n","\n","Please complete the following code to see if X and Y are independent for the given 2D Gaussian distribution (0.5pt)."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"XwvMvY0CvUYM","nbgrader":{"checksum":"849765b5e203f16e4ff88f06b7348f44","grade":false,"grade_id":"cell-2d716ff5b5854db8","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# P(y)P(x) - The independent approximation to P(x,y) (please try to use np.outer(...))\n","p_X_p_Y = None\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"O_PoNQuIRAFk","nbgrader":{"checksum":"d142c3c16ae5b65cd5a1f66db057aeb1","grade":true,"grade_id":"cell-af9634cf2d3bad5a","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# TEST YOUR ANSWER\n","assert p_X_p_Y.shape == (600, 600)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"DKPDLGR6vUYM","nbgrader":{"checksum":"fa9cf98efc0806fdabab62eb5d82d55c","grade":false,"grade_id":"cell-e937ae5d3b9ba5ed","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot_surface(x, y, p_X_p_Y, cmap='viridis', edgecolor='none')\n","ax.set_title(\"2D Gaussian Distribution\", fontsize=12, fontweight='bold')\n","ax.set_xlabel(\"X\", fontsize=12, fontweight='bold')\n","ax.set_ylabel(\"Y\", fontsize=12, fontweight='bold')\n","plt.xticks(fontsize=12, fontweight='bold')\n","plt.yticks(fontsize=12, fontweight='bold')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"69u4YfstvUYM","nbgrader":{"checksum":"4b23ff15323d3ac0fa9e7dde453166af","grade":false,"grade_id":"cell-286fe1fd574f142e","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## 2. Maximum Likelihood Estimation (MLE) & Maximum A Posterior (MAP)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"3KEN8RZzvUYM","nbgrader":{"checksum":"18f377e213ed86a240de375c4acbffd0","grade":false,"grade_id":"cell-fa8295eb49d1f82a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### 2.1 Maximum Likelihood Estimation (1pt)\n","\n","\n","In Lab 1, we used MLE for GLM. Here, we will start with a simpler case, using MLE to estimate a Poisson model where lambda is a free parameter not $\\theta x$, On class, we have known that `Poisson distribution` is one of the simplest models of neural spike counts. Specifically, suppose $x_{t} \\in \\mathbb{N}_{0}$ represents the number of spikes a neuron fires in time bin $t$, and we have $x_{t} \\sim Pois(\\lambda)$. The pmf of poisson distribution is given by\n","$$\n","Pois(x_{t}) = \\frac{\\lambda^{x_{t}}e^{-\\lambda}}{x_t!}\n","$$\n","\n","Assume $\\mathbf{x} = (x_{1}, x_{2}, ..., x_{T})$ denote the vector of spike counts. Then, the likelihood $p(\\mathbf{x})$ can be obtained as follows:\n","\n","$$\n","p(\\mathbf{x}) = \\prod_{t=1}^{T} p(x_t)\n","$$\n","$$\n","= \\prod_{t=1}^{T} \\text{Pois}(x_t)\n","$$\n","$$\n","= \\prod_{t=1}^{T} \\frac{1}{x_t!} \\lambda^{x_t} e^{-\\lambda}.\n","$$\n","\n","Then, our aim is to find the rate $\\lambda$ that maximizes the above likelihood, that is:\n","$$\n","\\lambda_{\\text{MLE}} = \\arg \\max p(\\mathbf{x})\n","$$\n","\n","which is the same as maximizing $log p(\\mathbf{x})$. Since log is a concave function, we can simply take the derivative of $log p(\\mathbf{x})$ w.r.t. $\\lambda$, and set it to zero, which is shown as follows:\n","\n","1) Write log likelihood:\n","$$\n","\\log p(\\mathbf{x}) = \\sum_{t=1}^{T} -\\log x_t! + x_t \\log \\lambda - \\lambda\n","$$\n","2) Take the derivative w.r.t. $\\lambda$:\n","$$\n","\\frac{\\text{d}}{\\text{d} \\lambda} \\log p(\\mathbf{x}) = \\sum_{t=1}^{T} \\left( \\frac{x_t}{\\lambda} - 1 \\right)\n","$$\n","$$\n","= \\frac{1}{\\lambda} \\left(\\sum_{t=1}^{T} x_t \\right) - T\n","$$\n","3) Set the derivative to zero and solve $\\lambda$:\n","$$\n","\\lambda_{\\text{MLE}} = \\frac{1}{T} \\sum_{t=1}^{T} x_t\n","$$\n","\n","According to the above results, it is easy to spot that `the MLE result of Poisson density is the mean of the empirical spike counts`. Based on this, please complete the following code to find $\\lambda_{\\text{MLE}}$ for the given neural spike data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hCCaKSzvUYM"},"outputs":[],"source":["if using_colab:\n","    f = h5py.File(\"/content/gdrive/MyDrive/lab3/data/whitenoise.h5\", mode='r')\n","else:\n","    f = h5py.File(\"./data/whitenoise.h5\", mode='r')\n","frame_rate = 100\n","dtype = torch.float32\n","times = torch.tensor(f['train']['time'][:], dtype=dtype)\n","stimulus = torch.tensor(f['train']['stimulus'][:], dtype=torch.uint8)\n","spikes = torch.tensor(f['train']['response']['binned'][:].T, dtype=dtype)\n","\n","# Get the size of the training data\n","num_frames, height, width = stimulus.shape\n","_, num_neurons = spikes.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"quHHJL_dvUYM","nbgrader":{"checksum":"3275fc527fc5dc52778bf3b874c0c206","grade":false,"grade_id":"cell-40e7baeb52b21907","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Plot a few seconds of the spike train\n","def plot_spike_train(spikes, t_start, t_stop, figsize=(12, 6)):\n","    \"\"\"\n","    `imshow` a window of the spike count matrix.\n","\n","    spikes:  time x neuron spike count matrix\n","    t_start: time (in seconds) of the start of the window\n","    t_stop:  time (in seconds) of the end of the window\n","    figsize: width and height of the figure in inches\n","    \"\"\"\n","    plt.figure(figsize=figsize)\n","\n","    plt.imshow(spikes[t_start*frame_rate:t_stop*frame_rate, :].T, aspect=20)\n","    data = spikes[t_start*frame_rate:t_stop*frame_rate, :].T\n","    np.save('spike', data.numpy())\n","\n","    return data\n","\n","spike_data = plot_spike_train(spikes, 0, 10)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"O3x8mIc2vUYN","nbgrader":{"checksum":"cc4c73377c0052401c3b9a2606f951aa","grade":false,"grade_id":"cell-ba35981b9858933a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Now, let us perform MLE for the given spike count dataset by assuming it follows Possion density. Please complete the following codes:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"dwuTopzevUYN","nbgrader":{"checksum":"e3970cc7e8542a1641dfa191cd11160c","grade":false,"grade_id":"cell-502982d9f9eff110","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# do not remove this line\n","np.random.seed(42)  # For reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"lG6BLGcmvUYN","nbgrader":{"checksum":"812d6d2db6b904f89b731b4a55533798","grade":false,"grade_id":"cell-0d7decf832e97077","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["estimated_rates = []\n","\n","# Iterate through each neuron and perform MLE\n","for neuron_idx in range(spike_data.shape[0]):\n","    estimated_lambda = None\n","    spike_counts = spike_data[neuron_idx].numpy()\n","\n","    # hint: The MLE result of Poisson density is the mean of the empirical spike counts\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    estimated_rates.append(estimated_lambda)\n","\n","    # Plot the spike histogram and the fitted Poisson distribution\n","    plt.figure(figsize=(4.5, 4))\n","    bins = torch.arange(15)\n","    pois = Poisson(estimated_lambda)\n","    plt.hist(spike_counts, bins, density=True, edgecolor='k', label='Spike Counts')\n","    plt.plot(bins + .5, torch.exp(pois.log_prob(bins)), '-o', label='pmf')\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"p(x)\")\n","    plt.title(f'Neuron {neuron_idx+1} - Estimated Poisson Rate: {estimated_lambda:.2f}')\n","    plt.grid()\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"iHTuaE6FvUYN","nbgrader":{"checksum":"bff42ffcfa1eed711f05617063ff6e3d","grade":false,"grade_id":"cell-6b78de67cbb41fa0","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### 2.2 Maximum A Posterior (1pt)\n","\n","The above MLE method does not take into account any prior information. If we have a sense for the distribution of neural firing rates based on our experience, we can encode this prior knowledge to the parameter estimation process, which is called MAP. For example, suppose we know that the rate $\\lambda$ follows `gamma distribution`: $\\lambda \\sim Ga(\\alpha, \\beta)$, where $\\alpha$ is the `shape` or `concentration` parameter, and $\\beta$ is the `inverse scale` or `rate` parameter. The pdf of gamma distribution is given as:\n","$$\n","\\text{Ga}(\\lambda) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}\n","$$\n","\n","where $\\Gamma(\\alpha)$ is the Gamma function, which generalizes the factorial function to continuous values. In a Bayesian context, when we do not know the rate $\\lambda$ of Poisson distribution, we can use Gamma distribution as `conjugate prior` for $\\lambda$. This means that if your prior belief about $\\lambda$ is described by a Gamma distribution and you observe data modeled by a Poisson distribution, the posterior distribution of $\\lambda$ (after observing the data) will also be a Gamma distribution. Now, let us consider the joint distribution of $\\mathbf{x}$ and $\\lambda$:\n","$$\n","p(\\mathbf{x}, \\lambda) = p(\\mathbf{x} \\mid \\lambda) p(\\lambda)\n","$$\n","$$\n","= \\left[ \\prod_{t=1}^{T} \\text{Pois}(x_t \\mid \\lambda) \\right] \\text{Ga}(\\lambda)\n","$$\n","\n","If we want to fit the rate $\\lambda$ under a gamma prior, we need to perform Bayesian inference by computing the posterior distribution of the rate $\\lambda$ given the observed spike counts $\\mathbf{x}$, which is given as:\n","$$\n","p(\\lambda | \\mathbf{x}) = \\frac{p(\\mathbf{x}, \\lambda)}{p(\\mathbf{x})}\n","$$\n","\n","Obviously, $p(\\mathbf{x})$ does not depend on $\\lambda$. Therefore, we can know that $p(\\lambda|\\mathbf{x})$ is proportional to $p(\\mathbf{x}, \\lambda)$. In this case, maximizing the posterior distribution is the same as maximizing the joint distribution, that is:\n","$$\n","\\lambda_{\\text{MAP}} = \\arg \\max p(\\mathbf{x}, \\lambda)\n","$$\n","\n","In this case, our aim converts to find the rate $\\lambda$ that maximizes the joint distribution $p(\\mathbf{x}, \\lambda)$, which can be expressed as:\n","$$\n","p(\\mathbf{x}, \\lambda) = C \\lambda^{\\alpha' - 1} e^{-\\beta' \\lambda}\n","$$\n","\n","where $\\alpha' = \\alpha + \\sum_{t=1}^{T} x_t$, $\\beta' = \\beta + T$, and $C$ is a constant w.r.t $\\lambda$. We can expand its pdf, take the log, take the derivative w.r.t. $\\lambda$, set it to zero and solve it as follows:\n","$$\n","\\lambda_{\\text{MAP}} = \\frac{\\alpha' - 1}{\\beta'} = \\frac{\\alpha - 1 + \\sum_{t=1}^{T} x_t}{\\beta + T}\n","$$"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"dhJ99bWYvUYN","nbgrader":{"checksum":"286960ad7608531ebf80a2705ecbf20a","grade":false,"grade_id":"cell-cce47c88d449faf8","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Now, let us perform MAP for the given spike count dataset by assuming that the neural firing rate follows gamma distribution. Please complete the following codes:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"zV4PQ_fAvUYN","nbgrader":{"checksum":"5522a86ab1a0d3290594eda4e234ad7d","grade":false,"grade_id":"cell-96d231996a13db61","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["estimated_rates = []\n","alpha_prior = 2\n","beta_prior = 1\n","\n","# Iterate through each neuron and perform MAP\n","for neuron_idx in range(spike_data.shape[0]):\n","    spike_counts = spike_data[neuron_idx].numpy()\n","\n","    # Step1: compute the summation of the spike count\n","    # Step2: find the total number of observations\n","    # Step3: update the estimated lambada in terms of the given formula\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    estimated_rates.append(lambda_map)\n","\n","    # Plot the spike histogram and the fitted Poisson distribution\n","    plt.figure(figsize=(4.5, 4))\n","    bins = torch.arange(15)\n","    pois = Poisson(lambda_map)\n","    plt.hist(spike_counts, bins, density=True, edgecolor='k', label='Spike Counts')\n","    plt.plot(bins + .5, torch.exp(pois.log_prob(bins)), '-o', label='pmf')\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"p(x)\")\n","    plt.title(f'Neuron {neuron_idx+1} - Estimated Poisson Rate: {lambda_map:.2f}')\n","    plt.grid()\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"YV8z_S9GYw25","nbgrader":{"checksum":"2b8a76e7503b1db374284de3ea68ffe4","grade":false,"grade_id":"cell-2b642d58e75602fe","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["#### Open Question 1 (1pt):\n","\n","The values of $\\alpha$ and $\\beta$ in the Gamma prior have a significant impact on the MAP estimation of $\\lambda$. These parameters define the prior distribution, determining the extent to which prior beliefs influence the estimate relative to the observed data.\n","\n","Please try two different settings of prior $\\alpha$ and $\\beta$ and analyze the results:\n","\n","- large $\\alpha$ and $\\beta$ (both parameters are significantly large);\n","- smaller $\\alpha$ and $\\beta$ (both parameters are close to zero).\n","\n","`Hint:` 1. Please focus more on the distribution difference between the original data and the estimated Poisson for the analysis part. 2. Using the following two cells to plot the results with different $\\alpha$ and $\\beta$ (you need to copy the code for MAP from previous cell).  \n","\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"id":"yAT37pzsbgeV","nbgrader":{"checksum":"2f917fdd24c4a255bdd063d78558d4de","grade":true,"grade_id":"cell-1716e0d8461d5704","locked":false,"points":1,"schema_version":3,"solution":true,"task":false}},"source":["<span style=\"color:red\">Your answer:</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"Sl0st0MnYwaf","nbgrader":{"checksum":"6c6343d7cd207f5796a805fd29108a38","grade":false,"grade_id":"cell-d931a68c0a295e6a","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Test large  𝛼  and  𝛽  (both parameters are significantly large)\n","\n","estimated_rates = []\n","\n","alpha_prior = None\n","beta_prior = None\n","\n","# assign both alpha and beta with very large values\n","alpha_prior = 10000\n","beta_prior = 20000\n","\n","# Iterate through each neuron and perform MAP\n","for neuron_idx in range(spike_data.shape[0]):\n","    spike_counts = spike_data[neuron_idx].numpy()\n","\n","    # Step1: compute the summation of the spike count\n","    # Step2: find the total number of observations\n","    # Step3: update the estimated lambada in terms of the given formula\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    estimated_rates.append(lambda_map)\n","\n","    # Plot the spike histogram and the fitted Poisson distribution\n","    plt.figure(figsize=(4.5, 4))\n","    bins = torch.arange(15)\n","    pois = Poisson(lambda_map)\n","    plt.hist(spike_counts, bins, density=True, edgecolor='k', label='Spike Counts')\n","    plt.plot(bins + .5, torch.exp(pois.log_prob(bins)), '-o', label='pmf')\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"p(x)\")\n","    plt.title(f'Neuron {neuron_idx+1} - Estimated Poisson Rate: {lambda_map:.2f}')\n","    plt.grid()\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"Nz_NqWmKbW0W","nbgrader":{"checksum":"6c2222779ea248e8bdd3b3ca1ce90dc4","grade":false,"grade_id":"cell-bb3ebc35701fe02b","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Test small  𝛼  and  𝛽  (both parameters are significantly large)\n","\n","estimated_rates = []\n","\n","alpha_prior = None\n","beta_prior = None\n","\n","# assign both alpha and beta with very small values (close to 0)\n","alpha_prior = 0.000001\n","beta_prior = 0.00003\n","\n","# Iterate through each neuron and perform MAP\n","for neuron_idx in range(spike_data.shape[0]):\n","    spike_counts = spike_data[neuron_idx].numpy()\n","\n","    # Step1: compute the summation of the spike count\n","    # Step2: find the total number of observations\n","    # Step3: update the estimated lambada in terms of the given formula\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    estimated_rates.append(lambda_map)\n","\n","    # Plot the spike histogram and the fitted Poisson distribution\n","    plt.figure(figsize=(4.5, 4))\n","    bins = torch.arange(15)\n","    pois = Poisson(lambda_map)\n","    plt.hist(spike_counts, bins, density=True, edgecolor='k', label='Spike Counts')\n","    plt.plot(bins + .5, torch.exp(pois.log_prob(bins)), '-o', label='pmf')\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"p(x)\")\n","    plt.title(f'Neuron {neuron_idx+1} - Estimated Poisson Rate: {lambda_map:.2f}')\n","    plt.grid()\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"r86XS70FvUYN","nbgrader":{"checksum":"8e5ed4472877f5cb44f1b3811c919b36","grade":false,"grade_id":"cell-1c3f31bbf33a7ef9","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## 3. Gaussian Mixture Models (GMMs) and Expectation-Maximization (EM)\n","\n","### 3.1 Introduction of GMMs and EM\n","\n","GMMs is a powerful tool for uncovering hidden structurs and latent patterns in complex neural data, such as spike trains or fMRI signals. it can represent different neural states or patterns as a mixture of several Gaussian distributions. For example, GMMs can be applied to cluster neural reponses or model the encoding of stimuli.\n","\n","Specifically, let $\\mathbf{x}_{i} \\in \\mathbb{R}^D$ denote the i-th observation, such as the population response or spike counts within a small time window. We use $\\mathbf{z}_{i} \\in \\{1, \\dots, K\\}$ denote the discrete latent state (aka cluster assignment) of that data point. For a data point $i$, its cluster assignment $z_{i}$ is generated based on a Categorical distribution determined by the mixture weights $\\pi$. The probability that data point $i$ belongs to cluster $k$ is $P(z_{i}|\\pi)=\\pi_{k}$. These $\\pi_{k}$ values are called mixture weights. And $\\sum_{k} \\pi_{k} =1$, where $0 \\leq \\pi_{k} \\leq 1 $.\n","\n","Once the cluster $k$ is decided for data point $i$, we can generate the data points $x_{i}$ based on a Gaussian distribution specific to that cluster , which has a mean $\\mu_{k}$ and a covariance matrix $\\sum_{k}$. That is, $x_{i}|z_{i} = k \\sim N(\\mu_{k}, \\sum_{k})$. Similarily, we want to use the MLE to find the parameters of each Gaussian distribution. By denoting the collection of all parameters to be estimated as $\\theta$, the log likelihood of GMM can be denoted as:\n","$$\n","\\begin{align*}\n","\\text{log likelihood}(\\theta) & = \\log P(X_1, \\dots, X_n = x_1, \\dots, x_n | \\theta) \\\\\n","& = \\sum_i \\log P(X_i = x_i | \\theta) \\\\\n","& = \\sum_i \\log \\sum_k P(X_i = x_i, Z_i = k | \\theta)\n","\\end{align*}\n","$$\n","\n","We know that we cannot pass the log through the sum, making it non-trivial to directly optimize the above log likehood expression. In fact, every time when we have a latent variable, the same issue will happen. In order to address this, we introduce Expectation-Maximization (EM) method. Let us rewrite the previous log likelihood expression as:\n","\\begin{align*}\n","\\text{log likelihood}(\\theta) & = \\sum_i \\log \\sum_k P(X_i = x_i, Z_i = k | \\theta) \\\\\n","& = \\sum_i \\log \\sum_k P(Z_i = k | x_i, \\theta) \\frac{P(X_i = x_i, Z_i = k | \\theta)}{P(Z_i = k | x_i, \\theta)} \\\\\n","& = \\sum_i \\log \\mathbb{E}_z \\left[ \\frac{P(X_i = x_i, Z_i = k | \\theta)}{P(Z_i = k | x_i, \\theta)} \\right]\n","\\end{align*}\n","\n","According to Jensen's inequality introduced in the lecture, we can find the lower bound of the above log likelihood as:\n","\n","\\begin{align*}\n","\\text{log likelihood}(\\theta) & = \\sum_i \\log \\mathbb{E}_z \\left[ \\frac{P(X_i = x_i, Z_i = k | \\theta)}{P(Z_i = k | x_i, \\theta)} \\right] \\\\\n","& \\geq \\sum_i \\mathbb{E}_z \\log \\left[ \\frac{P(X_i = x_i, Z_i = k | \\theta)}{P(Z_i = k | x_i, \\theta)} \\right] \\quad (\\text{Jensen's inequality}) \\\\\n","& = \\sum_i \\sum_k P(Z_i = k | x_i, \\theta) \\log \\left[ \\frac{P(X_i = x_i, Z_i = k | \\theta)}{P(Z_i = k | x_i, \\theta)} \\right]\n","\\end{align*}\n","\n","In this case, our aim changes to maximize this lower bound so that the log likelihood can also be optimized. Let us now apply EM to GMM:\n","\n","$\\bullet \\textbf{E-step:}$\n","\n","Using Bayes Rule, we have:\n","\\begin{align*}\n","P(Z_i = k | \\mathbf{x_i}, \\theta) &= \\frac{P(\\mathbf{X_i} = \\mathbf{x_i} | z_i = k, \\theta) P(Z_i = k | \\theta)}{P(\\mathbf{X_i} = \\mathbf{x_i} | \\theta)}\n","\\end{align*}\n","\n","In terms of the law of total probability, the denominator equals a sum over \\( k \\) of terms like those in the numerator. Then, we can calculate all of the terms as:.\n","\n","$$\n","\\begin{align*}\n","P(Z_i = k | \\mathbf{x_i}, \\theta) &= \\frac{N(\\mathbf{x_i}; \\mu_{k}, \\Sigma_{k}) \\pi_{k}}{\\sum_{k'} N(\\mathbf{x_i}; \\mu_{k'}, \\Sigma_{k'}) \\pi_{k'}} =: \\gamma_{ik}.\n","\\end{align*}\n","$$\n","\n","$\\bullet \\textbf{M-step:}$\n","\n","Maximizing the lower-bound:\n","\n","\\begin{align*}\n","\\max_{\\theta} \\sum_i \\sum_j \\gamma_{ik} \\log \\frac{P(X_i = x_i, Z_i = k \\mid \\theta)}{\\gamma_{ik}}.\n","\\end{align*}\n","\n","Because the term in the denominator does not depend on $\\theta$, we can remove it from the maximization process:\n","\\begin{align*}\n","\\max_{\\theta} \\sum_i \\sum_j \\gamma_{ik} \\log P(X_i = x_i, Z_i = k \\mid \\theta).\n","\\end{align*}\n","\n","For GMM, $\\theta$ represents three parameters: $\\pi$, $\\mu$, and $\\Sigma$. We can compute the updated parameters via taking the derivative w.r.t each parameter and set it 0. Then, the updated $\\pi'$, $\\mu'$, and $\\Sigma'$ can be expressed as:\n","\n","\\begin{align*}\n","\\mu_{k}' = \\frac{\\sum_i \\mathbf{x}_i \\gamma_{ik}}{\\sum_i \\gamma_{ik}}\n","\\end{align*}\n","\n","\\begin{align*}\n","\\Sigma_{k}' = \\frac{\\sum_i \\gamma_{ik} (\\mathbf{x}_i - \\mu_{k}')(\\mathbf{x}_i - \\mu_{k}')^T}{\\sum_i \\gamma_{ik}}\n","\\end{align*}\n","\n","\\begin{align*}\n","\\mathcal{w}_{k} = \\frac{\\sum_i \\gamma_{ik}}{n}\n","\\end{align*}\n","\n","\\begin{align*}\n","N_{k} = \\sum_i \\gamma_{ik}\n","\\end{align*}\n","\n","where n is the number of total samples, and $N_{k}$ is the effective number of data points associated with component k .\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"y9GAfnFAgNBK","nbgrader":{"checksum":"bc39ab796e1dd903fcdb97722a593c04","grade":false,"grade_id":"cell-ea21b5d42a2afc59","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### 3.2 Apply GMMs and EM on Rat hippocampal CA1 data\n","\n","Now, you already know the theory of GMM and how to use EM to estimate the parameters. Let us apply GMM and EM to a public rat's hippocampus dataset: [Rat hippocampal CA1 data](https://www.nature.com/articles/s42003-023-04958-0). The Rat Hippocampal CA1 data was collected from 120 putative pyramidal neurons in the CA1 region of the hippocampus while a rat ran on a 1.6-meter linear track with rewards at both ends. Neural activity was recorded as the rat traversed the track, with one lap defined as running from one end to the other, resulting in a total of 84 laps. The `rat's position and running direction` were treated as continuous `labels` $u$, and the `recorded spike activities` were treated as the `observation` $x$, which were was binned into 25ms intervals to capture the ensemble dynamics. The illustration of the above experiment scenairo for collecting Rat Hippocampal CA1 data is shown in Figure 1.\n","\n","<center><img src=\"https://github.com/ZKBig/8803_GMM_Lab3/blob/main/CA1_data.png?raw=true\" width=\"600\" height=\"200\"/></center>\n","<!-- ![CA1_data](https://drive.google.com/uc?id=1y3QhKffqte7DbZmmXJFt2VC4zjbwrLGD) -->\n","<center> Figure 1: Linear track and sample running path. The two ends are labeled as L & R. Two directions are color-coded by red and blue, and positions are coded by color saturation. </center>\n","\n","By inputing the spike activities to the VAE model, we obtain the latent representation of the rat's position and direction. using the latent variables instead of the raw spike activites is beneficial because VAE has already condensed the high-dimensional and complex neural activity into a more structured, interpretable latent space. In this latent space, the representations of the rat's position and direction are more separable, making it easier for us to distinguish the two directions and various positions. In this lab, we use an effective VAE model named [pi-VAE](https://arxiv.org/abs/2011.04798). `We provide the latent representation to you` sothat you can directly apply GMM and EM to identify the rat's position and direction pattern."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_tgJmjPvUYO"},"outputs":[],"source":["# load the latent variables\n","if using_colab:\n","    data = loadmat(\"/content/gdrive/MyDrive/lab3/data/CA1_latent_variables.mat\")['data']\n","else:\n","    data = loadmat('./data/CA1_latent_variables.mat')['data']"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"a3jemJImvUYO","nbgrader":{"checksum":"ea423c72e06a5ca89e8db06575a31fdf","grade":false,"grade_id":"cell-38f2f94ac892d551","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Plot the latent variables\n","# The L and R in the figure indicates the two ends (right end and left end)\n","plt.figure(figsize=(4.5, 4))\n","plt.scatter(data[:, 0], data[:, 1], s=2, color='green', alpha=0.4)\n","plt.text(1, 2, 'L', fontsize=18, fontweight='bold', color='black')\n","plt.text(-1.5, -5, 'R', fontsize=18, fontweight='bold', color='black')\n","plt.xlabel(\"Latent 1\")\n","plt.ylabel(\"Latent 2\")\n","plt.title(\"Latent (pi-VAE)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"gL86kuDxvUYO","nbgrader":{"checksum":"762430b89156f96b9b2344c9ae2d4d24","grade":false,"grade_id":"cell-0a2e9c51322a976b","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Initialization of parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"qkQZ5LTIvUYO","nbgrader":{"checksum":"4ec5fe0b6e8a2d6fb60089490ac44c64","grade":false,"grade_id":"cell-8ebe843241578a95","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["np.random.seed(42)  # For reproducibility\n","def multivariate_gaussian(x, mean, cov):\n","    return multivariate_normal.pdf(x, mean=mean, cov=cov)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"2H70XSrmvUYO","nbgrader":{"checksum":"466ab24935d08591f97751b9b808f31d","grade":false,"grade_id":"cell-8ed46f15fc351991","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["K = 2 # number of components\n","n, d = data.shape # get the number of points and dimensionality\n","\n","# 1. Initialize means from the data\n","u_s = data[np.random.choice(n, K, False)]\n","# 2. Initialize covariances as identity matrices\n","covariances = np.array([np.eye(d)] * K)\n","# 3. Initialize mixture weights uniformly\n","pi_s = np.ones(K) / K\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"lqr2x92vvUYO","nbgrader":{"checksum":"f2a97a7331a276abc705d8b9311d4e4b","grade":false,"grade_id":"cell-25ad847eb69c67ad","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["### Run the EM process (3pt)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"Z9DmF3Y6vUYO","nbgrader":{"checksum":"9e5d6474db0bde2586ced7dbfb15a840","grade":false,"grade_id":"cell-f1e6f17374ce3784","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["posterior = np.zeros((n, K)) # initizalize posterior probabilities\n","threshold = 1e-7 # set the threshold for the end of EM\n","max_iteration = 100 # set the maximum iteration value\n","prev_log_likelihood = -np.inf\n","\n","for step in range(max_iteration):\n","    # 1. E-step:\n","    # 1.1 compute the posterior probabilities for each component\n","    # Hint for 1.1: use for-loop and multivariate_gaussian()\n","    # 1.2 Normalize the posterior probabilities\n","    # Hint for 1.2: use posterior.sum(...) to compute the summation of posterior probabilities for all components\n","    # The codes should be less than 5 lines\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    # 2. M-step:\n","    # 2.1 compute the effective number of data points associated with component k\n","    # Hint for 2.1: use posterior.sum(...)\n","    # This should be a one-line code\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    # 2.2 update means\n","    for k in range(K):\n","        u_s[k] = (posterior[:, k][:, np.newaxis] * data).sum(axis=0) / N_k[k]\n","\n","    # 2.3 update covariances\n","    # Hint for 2.3: use for-loop and np.dot(...)\n","    # The codes should be less than 5 lines\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    # 2.4 update mixture weights\n","    # This should be a one-line code\n","    # YOUR CODE HERE\n","    raise NotImplementedError()\n","\n","    log_likelihood = 0\n","    for k in range(K):\n","        log_likelihood += np.sum(np.log(multivariate_gaussian(data, u_s[k], covariances[k])) * posterior[:, k])\n","\n","    if abs(log_likelihood - prev_log_likelihood) < threshold and step > 0:\n","        print(f\"The EM process Converges after {step} steps.\")\n","        break\n","\n","    prev_log_likelihood = log_likelihood"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"rtXm7p-8vUYO","nbgrader":{"checksum":"50ceb43b57dce94eec93580222a8c2af","grade":false,"grade_id":"cell-4a3af207aa0abf0e","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["###  Plot and analyze the resulting clusters of the rat's position and direction pattern (1.5pt)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"3JqK7tM9vUYO","nbgrader":{"checksum":"871837df4338e20a37e56a045ced1aff","grade":false,"grade_id":"cell-c7b03c06a2635d4a","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["labels = np.argmax(posterior, axis=1)\n","plt.figure(figsize=(4.5, 4))\n","plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=2, alpha=0.4)\n","plt.text(1, 2, 'L', fontsize=18, fontweight='bold', color='black')\n","plt.text(-1.5, -5, 'R', fontsize=18, fontweight='bold', color='black')\n","plt.xlabel(\"Latent 1\")\n","plt.ylabel(\"Latent 2\")\n","plt.title(\"Clustered Latent (pi-VAE) via GMM-EM\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"id":"tCf-gNiTs-pj","nbgrader":{"checksum":"009faa10db8343d57cb3cc11473e2763","grade":true,"grade_id":"cell-7497f07f10a0d8f8","locked":false,"points":1.5,"schema_version":3,"solution":true,"task":false}},"source":["\n","#### Open Question 2:\n","Please analyze the above clustering result via GMM and EM. For example, does the results make sense to you? what can you learn from the results?\n","\n","<span style=\"color:red\">Your answer:</span>"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}